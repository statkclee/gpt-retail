# 데이터 과학 {#sec-data-science}

데이터 과학을 설명하는 용어들이 넘쳐난다. DIKW 피라미드, CRISP-DM, 의사결정 연속체(continuum), OSEMN, MLOps... 각각은 데이터 과학의 중요한 측면을 다루지만, 이들이 어떻게 연결되는지는 명확하지 않다. "DIKW와 CRISP-DM은 무슨 관계인가?", "MLOps는 별개의 프레임워크인가?" 같은 질문들이 실무자와 학습자를 혼란스럽게 만든다.

이 장에서는 **세 가지 질문**을 통해 데이터 과학 전체 지형도를 이해한다. 첫째, **"무엇이 가치인가?"**(DIKW 피라미드). 둘째, **"어떻게 가치를 추출하는가?"**(KDD, CRISP-DM, OSEMN, MLOps 등 시대별 진화). 셋째, **"어떻게 가치를 활용하는가?"**(의사결정 vs 데이터 제품). 이 질문들은 불변의 본질과 가변의 도구를 구분하고, 각 프레임워크가 어디에 위치하는지 명확히 보여준다.


## DIKW 피라미드 {#sec-dikw-hierarchy}

데이터 과학 본질은 **데이터를 지혜로 변환**하는 것이다. 가장 영향력 있는 프레임워크가 러셀 애코프(Russell Ackoff)가 1989년 제시한 **DIKW 계층 구조(DIKW Hierarchy)**다 [@ackoff1989dikw].

![DIKW 계층 구조: 데이터에서 지혜로의 변환](images/ds-continuum-dikw-hierarchy.svg){#fig-dikw-hierarchy fig-align="center"}

@fig-dikw-hierarchy 는 데이터, 정보, 지식, 지혜의 4단계 계층과 각 단계 간 변환 과정을 시각화한다. 피라미드 구조는 각 단계로 올라갈수록 **볼륨은 감소하지만 가치와 복잡도는 증가**한다는 핵심 특성을 나타낸다.

DIKW 계층을 이해하는 핵심은 각 단계의 고유한 특성과 단계 간 변환 과정이다. 가장 하위 계층인 **데이터**는 숫자, 기호, 관찰 결과 등 가공되지 않은 원시 사실이다. "38.5", "5천만 원", "78점" 같은 개별 값들이 데이터다. 그 자체로는 의미나 해석이 없으며, 저장, 전송, 처리가 가능한 기본 단위다.

데이터에 맥락, 관련성, 목적이 부여되면 **정보**가 된다. 정보는 "무엇? 언제? 어디?"에 답하며, 불확실성을 감소시키고 커뮤니케이션을 가능하게 한다. "체온 38.5°C는 정상보다 1°C 높음 → 발열 진단"처럼 원시 데이터에 맥락을 부여한 것이 정보다. 정보는 이해를 위해 조직되고 구조화되며, 의사결정을 지원한다.

정보에 경험과 학습이 결합되면 **지식**이 형성된다. 지식은 "어떻게? 만약에?"에 답하며, 문제 해결과 예측을 가능하게 한다. "발열+정상맥박 패턴은 바이러스 감염을 시사한다"는 진술은 개별 정보를 넘어 전문가의 경험과 학습을 통해 축적된 지식이다. 지식은 실습과 반복을 통해 구축되며, 적용과 예측을 가능하게 한다.

최상위 계층인 **지혜**는 지식에 가치, 윤리, 판단력이 결합된 상태다. 지혜는 "왜 해야 하는가? 무엇이 옳은가?"에 답하며, 장기적 결과를 고려한 건전한 의사결정을 가능하게 한다. "환자 복지를 고려할 때 항생제보다는 휴식을 권장한다"는 판단은 의학 지식에 환자 중심 가치와 윤리적 고려를 결합한 지혜다. 지혜는 도덕적, 윤리적 차원을 포함하며, 건전한 판단과 의사결정을 가능하게 한다.

![DIKW 계층의 실전 사례](images/ds-continuum-dikw-examples.svg){#fig-dikw-examples fig-align="center"}

@fig-dikw-examples 는 네 가지 도메인에서 DIKW 변환이 실제로 어떻게 작동하는지 보여준다. 의료에서 "체온 38.5°C"는 "발열 진단"(정보)을 거쳐 "바이러스 감염 패턴"(지식)으로, 최종적으로 "휴식 권장, 항생제 불필요"(지혜)로 완성된다. 비즈니스에서는 "매출 5천만 원"이 "목표 대비 20% 부족"(정보), "계절적 수요 감소"(지식), "4분기 전략 조정"(지혜)으로 이어진다. 교육에서는 "시험 점수"가 "평균 77.8점, 변동성 큼"(정보), "특정 개념 부족"(지식), "맞춤형 보충 수업"(지혜)으로 발전하고, 과학 연구에서는 "CO₂ 421ppm"이 "역사상 최고치"(정보), "기후 변화 상관관계"(지식), "2050 탄소중립 정책"(지혜)으로 변환된다. 네 영역 모두 동일한 패턴을 따른다. **원시 사실(데이터) → 맥락 부여(정보) → 패턴 인식(지식) → 가치 판단(지혜)**.


### AI 시대 계층 간 변환

DIKW 계층 핵심은 단계 간 **변환 과정**이다. 데이터에서 정보로 변환은 **처리(Processing)**를 통해 이루어진다. 맥락과 관련성, 목적을 부여하는 과정이다. 수집, 정제, 맥락 구축이 핵심이다. 정보에서 지식으로의 변환은 **학습(Learning)**을 통해 일어난다. 경험, 기술, 이해를 축적하는 과정이다. 실습과 반복, 패턴 인식이 중요하다. 지식에서 지혜로의 변환은 **통찰(Insight)**을 통해 완성된다. 가치, 판단, 윤리를 결합하는 과정으로, 성찰과 윤리적 고려가 필수적이다.

DIKW 프레임워크는 1989년 제시되었지만, AI 시대에 오히려 더욱 중요해졌다. 현대 조직은 **데이터 과잉** 상황에 직면해 있다. 센서, 로그, 트랜잭션 등에서 생성되는 방대한 원시 데이터는 저장과 처리는 쉬워졌지만, 이를 의미 있는 정보로, 다시 실행 가능한 지식과 지혜로 변환하는 것은 여전히 어렵다.

AI, 특히 대규모 언어 모델의 진정한 가치는 **DIKW 변환 과정 자동화와 가속화**에 있다. ChatGPT나 Claude 같은 도구는 **데이터 → 정보** 변환에서 원시 데이터를 읽고 맥락을 파악하여 의미 있는 정보로 구조화한다. **정보 → 지식** 변환에서는 패턴을 인식하고 방대한 학습 데이터를 바탕으로 이해와 예측을 제공한다. **지식 → 지혜** 변환에서는 다양한 관점과 윤리적 고려사항을 통합하여 균형 잡힌 판단을 지원한다.

그러나 중요한 것은, AI가 각 계층을 완전히 대체하는 것이 아니라 **인간의 DIKW 상승을 지원하는 도구**라는 점이다. 최종적인 가치 판단과 윤리적 의사결정은 여전히 인간의 영역이며, AI는 그 과정을 가속화하고 확장하는 역할을 한다.

## 기술 방법론 진화 {#sec-technical-methods}

DIKW가 가치의 본질을 정의한다면, 기술 방법론은 **"어떻게 데이터를 가치 있는 인사이트로 변환하는가"**에 답한다. 환경과 기술 변화에 따라 방법론도 진화해왔다. 1990년대부터 현재까지 데이터 과학 방법론 3세대 진화가 @fig-methodology-evolution 에 제시되어 있다. 각 세대는 시대적 환경과 기술 변화에 대응하며 발전했다. 1세대(1990년대)는 비즈니스 중심 전통적 데이터 마이닝으로 CRISP-DM, SEMMA, KDD가 대표적이다. 2세대(2000-2010년대)는 코드 중심 데이터 과학으로 OSEMN과 Tidyverse 워크플로우가 등장했다. 3세대(2014년 이후)는 운영 및 제품화 중심으로 MLOps/DataOps가 표준이 되었다. 각 세대는 이전 세대를 대체하지 않고 보완하며 공존한다.

![데이터 방법론 진화 타임라인](images/ds-continuum-methodology-evolution.svg){#fig-methodology-evolution fig-align="center"}



### 비즈니스 중심 데이터 마이닝

1990년대는 데이터베이스 보급 초기로 학계 중심 연구와 기업 CRM/ERP 시스템이 등장하던 시기다. 주도 기술은 SQL, SAS, SPSS 등 전통적 통계 소프트웨어였으며, GUI 중심의 분석 도구가 주류를 이뤘다. 핵심은 **이론적 엄밀성과 비즈니스 중심 접근**이었다.

**KDD(Knowledge Discovery in Databases)**는 1996년 우사마 파야드(Usama Fayyad) 등이 제시한 학술적 프레임워크[@fayyad1996kdd]로 데이터 마이닝 이론적 기초를 확립했으며 5단계로 구성된다: 선택(Selection) → 전처리(Preprocessing) → 변환(Transformation) → 데이터 마이닝(Data Mining) → 해석/평가(Interpretation/Evaluation). 학술 논문 중심으로 발전하며 데이터에서 지식을 발견하는 체계적 방법론을 정립했다.

**CRISP-DM(Cross-Industry Standard Process for Data Mining)**은 1996년 시작되어 2000년대 산업 표준으로 자리 잡았으며 6단계 순환 구조로 구성된다: 비즈니스 이해(Business Understanding) → 데이터 이해(Data Understanding) → 데이터 준비(Data Preparation) → 모델링(Modeling) → 평가(Evaluation) → 배포(Deployment). KDD와 달리 순환적 구조를 강조하며, 비즈니스 목표에서 시작해 다시 비즈니스 가치로 돌아가는 완전한 프로젝트 생명주기를 다룬다. **현재까지도 많은 데이터 마이닝 프로젝트에서 방법론을 사용**하며, 사실상 산업 표준으로 자리 잡았다.

**SEMMA(Sample, Explore, Modify, Model, Assess)**는 SAS Institute가 제시한 또 다른 대표적 방법론이다. 표본 추출 → 탐색 → 수정 → 모델링 → 평가 5단계로 구성되며, SAS Enterprise Miner와 같은 GUI 도구와 긴밀히 통합되어 실무자들에게 널리 사용되었다.

1세대 방법론의 공통점은 **표준화된 단계와 비즈니스 중심 접근**이다. 데이터 준비가 전체 시간의 50-70%를 차지하며, 최종 결과물은 예측 모델, 데이터 인사이트, 비즈니스 추천사항이었다. GUI 중심 도구(SAS Enterprise Miner, SPSS Modeler)가 주류를 이루며 위험 관리와 표준화를 중시했다.

### 코드 중심 데이터 과학

2000년대부터 2010년대 중반은 하둡, 스파크 등 빅데이터 기술이 등장하고 Python과 R 생태계가 폭발적으로 성장한 시기다. 주도 기술은 Python(pandas, scikit-learn, numpy)과 R(dplyr, ggplot2, tidymodels)이며, 오픈소스와 코드 기반 접근이 주류를 이뤘다. 핵심은 **실용성, 유연성, 재현 가능성**이었다.

**OSEMN**(발음: "Awesome")은 힐러리 메이슨(Hilary Mason)과 크리스 위긴스(Chris Wiggins)가 2010년 제시한 5단계 프레임워크[@mason2010osemn]로 CRISP-DM보다 훨씬 간결하며 실무자 중심으로 설계되었다: 획득(Obtain) → 정제(Scrub) → 탐색(Explore) → 모델링(Model) → 해석(iNterpret). 각 단계는 명확하고 실행 가능하다. "Obtain"에서는 API, 크롤링, DB 등에서 데이터를 수집하고, "Scrub"에서는 결측치 처리와 이상치 제거를 수행한다. "Explore"에서는 EDA를 통해 패턴을 발견하고, "Model"에서는 머신러닝 알고리즘을 적용하며, "iNterpret"에서는 결과를 해석하고 인사이트를 도출한다. Python 중심 데이터 과학 커뮤니티에서 널리 사용되며, Jupyter Notebook과 함께 빠른 실험과 반복을 가능하게 했다.

**Tidyverse**는 해들리 위컴(Hadley Wickham)이 주도한 R 중심의 데이터 과학 워크플로우다 [@wickham2014tidy]. 2014년 dplyr 출시 이후 R 커뮤니티 사실상 표준이 되었으며 6단계로 구성된다: 가져오기(Import) → 정돈(Tidy) → 변환(Transform) ↔ 시각화(Visualize) ↔ 모델링(Model) → 커뮤니케이션(Communicate). OSEMN과 달리 순환적 반복 구조를 강조한다. "Transform ↔ Visualize ↔ Model"은 상호작용하며 반복되는 탐색 과정을 나타낸다. **Tidy Data 원칙**(각 변수는 열, 각 관측치는 행, 각 값은 셀)을 기반으로 한 철학적 일관성이 특징이다. readr, tidyr, dplyr, ggplot2, tidymodels 등 모든 패키지가 동일한 설계 철학을 공유하며, Rmarkdown/Quarto로 재현 가능한 데이터 과학 산출물을 만들어 낸다.

2세대 방법론의 공통점은 **코드 우선, 오픈소스, 재현 가능성**이다. GUI 도구 대신 Jupyter Notebook, RStudio 같은 코드 중심 환경을 사용하며, Git으로 버전 관리하고, 오픈소스 라이브러리 생태계를 적극 활용한다. 1세대 표준화된 단계보다는 빠른 실험과 반복적 개선을 중시한다. 협업과 공유가 용이하며, 전 세계 데이터 과학 커뮤니티가 코드와 노하우를 공유하며 발전했다.

### 운영 및 제품화 중심 AI

2014년 이후는 머신러닝 모델이 실험실을 넘어 핵심 비즈니스가 되면서 24/7 운영과 자동화가 필수가 된 시기다. 주도 기술은 Docker, Kubernetes, MLflow, Airflow, dbt 등 DevOps와 통합된 도구들이며, 클라우드 네이티브와 컨테이너화가 표준이 되었다. 핵심은 **자동화, 확장성, 신뢰성**이었다.

**MLOps(Machine Learning Operations)**는 머신러닝 모델의 프로덕션 배포 및 운영을 자동화하는 프레임워크다. DevOps 원칙을 머신러닝에 적용한 것으로, 모델 개발과 운영 간극을 메우며, 핵심 구성 요소는 다음과 같다: 모델 레지스트리(MLflow로 버전 관리 및 추적), CI/CD 파이프라인(GitHub Actions, Jenkins로 자동 배포), 모델 서빙(FastAPI, TensorFlow Serving으로 API 제공), 모니터링(Prometheus, Grafana로 성능 모니터링), 드리프트 감지(Evidently AI로 데이터/모델 드리프트 탐지), 자동 재학습(Airflow로 주기적 재학습 파이프라인). MLOps 목표는 모델을 빠르고 안정적으로 프로덕션에 배포하고, 지속적으로 모니터링하며, 성능 저하 시 자동으로 재학습하는 것이다.

**DataOps(Data Operations)**는 데이터 파이프라인 품질과 신뢰성을 보장하는 프레임워크다. MLOps가 모델에 집중한다면, DataOps는 데이터 흐름 전체에 집중하며, 핵심 구성 요소는 다음과 같다: 데이터 파이프라인(Airflow, Prefect로 ETL 자동화), 데이터 품질(Great Expectations로 품질 검증), 데이터 카탈로그(DataHub, Amundsen으로 메타데이터 관리), 데이터 리니지(데이터 흐름 추적 및 영향도 분석), 데이터 거버넌스(RBAC, 데이터 마스킹, 감사 로그). DataOps 목표는 데이터 정확성, 적시성, 접근성을 보장하고, 데이터 팀 간 협업을 원활하게 하는 것이다.

3세대 방법론의 공통점은 **Infrastructure as Code, 자동화, 운영 통합**이다. 모든 인프라를 코드로 관리하고(Terraform, CloudFormation), 컨테이너로 패키징하며(Docker), 오케스트레이션 도구로 배포한다(Kubernetes). 2세대 실험적 접근을 넘어 프로덕션 안정성과 확장성을 중시한다. 모델과 데이터가 비즈니스의 핵심 자산이 되면서, 이를 관리하고 운영하는 체계적 프레임워크가 필수가 되었다.


### 데이터 과학 워크플로우 {#sec-workflow}

위에서 살펴본 방법론들의 공통 패턴을 추상화하면 데이터 과학의 일반적 워크플로우를 도출할 수 있다. @fig-data-science-workflow 는 현실 세계에서 시작하여 다시 현실 세계로 돌아가는 순환적 프로세스를 시각화한다. 전체 워크플로우는 **인사이트 생산 단계(1-6단계)**와 **가치 실현 분기(7-8단계)**로 구성된다.

![데이터 과학 워크플로우: 순환적 프로세스](images/ds-continuum-data-science-workflow.svg){#fig-data-science-workflow fig-align="center"}

**인사이트 생산의 핵심 흐름**은 원시 데이터 수집에서 시작한다. 현실 세계에서 수집된 데이터는 클리닝과 변환 과정을 거쳐 정제된 데이터로 준비되며, 프로젝트 시간의 대략 60-80%를 차지한다. 정제된 데이터는 탐색적 데이터 분석(EDA)을 통해 패턴을 발견하고 가설을 형성하며, 머신러닝 알고리즘과 통계 모델을 통해 예측력을 확보한다. 최종적으로 시각화와 보고서로 인사이트를 전달하며, 이로써 **"데이터를 인사이트로 변환"**하는 기술적 프로세스가 완료된다.

**가치 실현의 두 가지 분기**는 커뮤니케이션 단계 이후에 나타난다. 첫 번째 경로는 **의사결정 수행**으로, 분석 결과를 바탕으로 조직의 전략적 의사결정과 실행으로 이어진다. 두 번째 경로는 **데이터 제품 구축**으로, 분석 로직을 자동화된 서비스로 전환하여 지속적 가치를 제공한다. 두 경로는 독립적이면서 상호보완적이며, 대부분의 프로젝트는 두 경로를 병렬로 진행하여 가치를 극대화한다. 모든 결과는 점선 화살표로 표시된 것처럼 다시 현실 세계로 피드백되어 새로운 데이터 수집과 개선 순환을 시작한다.

## 의사결정과 제품화 {#sec-value-utilization}

기술 방법론(CRISP-DM, OSEMN 등)으로 **인사이트를 생산**했다. 이제 인사이트를 **현실 세계에 어떻게 적용**하는가?

데이터 과학 프로젝트는 두 가지 형태로 가치를 창출한다. **경로 A(의사결정)**는 인사이트를 조직의 전략적 결정과 실행으로 전환하는 방식이다. 분석가가 고객 이탈 패턴을 발견하면, 경영진은 이를 바탕으로 VIP 고객팀 신설을 결정하고, 예산을 배정하며, 조직을 재편한다. 이는 일회성 의사결정이거나 분기별 전략 회의처럼 정기적으로 반복될 수 있다. **경로 B(데이터 제품 구축)**는 분석 로직을 자동화된 시스템으로 구축하여 지속적 서비스를 제공하는 방식이다. 같은 이탈 모델을 프로덕션 환경에 배포하면, 매일 아침 자동으로 이탈 위험 고객 목록을 생성하고, CRM에 스코어를 표시하며, 담당자에게 알림을 보낸다. 인간 개입 없이 24시간 가치를 창출하는 것이다. 두 경로는 배타적이지 않으며, 대부분의 성공적인 프로젝트는 두 경로를 병렬로 진행한다.

### 데이터 기반 의사결정 연속체 {#sec-decision-continuum}

**데이터 기반 의사결정 연속체**(Data-Informed Decision-Making Continuum)는 조직 전체가 데이터 인사이트를 비즈니스 액션으로 전환하는 프로세스다. CRISP-DM 등 기술 방법론이 **인사이트를 생산**했다면, 7단계 연속체는 **조직의 의사결정으로 전환**한다. @fig-continuum-traditional 는 획득(Acquire)부터 실행(Take Action)까지 7단계의 선형적 프로세스를 시각화한다. 전체 사이클 완료에 2-6개월이 소요되며, 각 단계는 특정 활동과 소요 시간을 가진다.

![데이터 기반 의사결정 연속체](images/ds-continuum-traditional.svg){#fig-continuum-traditional fig-align="center"}

**전반부 4단계는 인사이트 생산에 집중**한다. 획득 단계에서 데이터베이스 쿼리, API 연결, 파일 가져오기 등을 통해 원천 데이터를 확보하며 며칠에서 몇 주가 소요된다. 통합 단계에서는 데이터 클리닝, ETL 프로세스, 스키마 매핑으로 여러 소스를 결합하고 품질을 검증하며 역시 며칠에서 몇 주가 필요하다. 노출 단계에서는 대시보드 생성과 쿼리 인터페이스를 통해 조직 내 접근을 가능하게 하며, 보고 단계에서는 통계 분석과 KPI 계산으로 트렌드를 식별한다. 의사결정 연속체 앞부분 4단계는 앞서 설명한 기술 방법론(CRISP-DM 초반부)과 내용이 중복되지만, 관점이 다르다. 기술 방법론이 데이터 과학자 관점에서 Python/R 코드 작성에 집중한다면, 의사결정 연속체는 조직 전체 관점에서 여러 팀 협업을 강조한다.

**후반부 3단계가 연속체 핵심**이며, CRISP-DM에는 없는 단계로 기술적 인사이트를 비즈니스 결정으로 전환한다. 스토리텔링 단계에서는 분석 결과를 맥락 속에 배치하고 서사를 구성한다. 예를 들어 "고객 이탈 모델이 85% 정확도로 예측했으며, 첫 구매 후 30일 이내 재구매하지 않으면 80%가 이탈합니다. 작년 이탈 고객 3만 명의 생애가치는 150억 원이며, VIP 고객 전담팀 신설 시 30% 이탈 방지로 연간 45억 원 매출 보존 효과가 예상됩니다"와 같이 데이터에 의미를 부여한다. 결정 단계에서는 경영진이 옵션 평가, 위험 평가, 이해관계자 의견 수렴, 비용-편익 분석을 거쳐 최종 선택을 내린다(VIP 고객팀 5명 신설, 연간 2억 원 예산 승인, 마케팅팀 산하 배치, 이탈률 30% → 21% 목표 설정 등). 이 단계는 며칠에서 몇 주가 소요되며, 데이터 과학자는 조언자 역할만 하고 경영진이 최종 결정을 내린다는 점에서 기술 방법론과 구별된다. 실행 단계에서는 결정사항을 현실화한다. VIP팀 채용, 신규 고객 30일 내 자동 연락 프로세스 수립, CRM에 이탈 스코어 표시, 월별 이탈률 대시보드 운영 등 자원 배분, 프로세스 변경, 시스템 업데이트, 팀 조율, 결과 모니터링이 며칠에서 몇 개월에 걸쳐 이루어진다. CRISP-DM 배포가 기술적 배포만 다루는 것과 달리, 조직적 실행을 포함한다.

이러한 선형적 7단계 프로세스는 체계적이고 검증 가능하다는 장점이 있지만, 현대 비즈니스 환경에서 심각한 한계를 드러낸다. 시간 지연 문제가 가장 크다. 전체 사이클 완료까지 2-6개월이 소요되는 동안 비즈니스 환경은 이미 변화한다. 특히 전자상거래, 금융, 소셜미디어 같은 빠르게 변하는 산업에서는 의사결정이 완료되는 시점에 이미 기회를 놓치거나 위험이 현실화된다. 단절된 워크플로우도 문제다. 각 단계가 서로 다른 팀과 도구로 분리되어 있어, 커뮤니케이션 비용이 증가하고 정보 손실이 발생한다. 인간 의존성은 확장성을 제한한다. 모든 단계가 인간의 수동 개입을 필요로 하므로, 처리할 수 있는 의사결정의 수와 속도에 본질적 한계가 있다.

![데이터 기반 의사결정의 진화](images/ds-continuum-evolution.svg){#fig-continuum-evolution fig-align="center"}

@fig-continuum-evolution 은 1990년대부터 현재까지 데이터 기반 의사결정 프로세스의 진화를 시각화한다. **전통적 시대(1990년대-2000년대)**는 획득-통합-노출-보고-스토리텔링-결정-실행의 7단계를 순차적으로 진행했으며, 타임라인은 2-6개월, 수동 프로세스, 직관 기반 의사결정으로 특징지어진다. **데이터 과학 시대(2010년대-2020년)**는 단계를 통합하고 가속화했다. 획득과 통합은 자동화 ETL로 병합되었고, 노출과 보고는 대화형 분석으로, 스토리텔링은 증거 기반 내러티브로, 결정과 실행은 모델 기반 예측적 의사결정으로 발전했다. 타임라인은 며칠에서 몇 주로 단축되었고, 반자동화되었으며, 가설 주도 의사결정이 가능해졌다. 머신러닝 모델, A/B 테스팅, 실시간 대시보드, 예측 분석, 통계적 검정, Python/R/SQL 같은 새로운 역량이 등장했다.

데이터 과학 시대는 의사결정 연속체를 현대화했으나 원칙을 훼손하지는 않았다. 더 빠르고, 더 스마트하며, 더 자동화되었지만 여전히 근본적으로 모든 것은 인간이 통제한다. 의사결정 7단계 연속체의 개념적 프레임워크는 여전히 유효하며, 기술은 프로세스를 자동화하고 가속화할 뿐 본질적 구조를 대체하지 않는다. 특히 결정과 실행 단계는 여전히 인간의 영역이다. 패러다임 전환은 AI 시대 자율적 의사결정과 함께 도래했다. AI, 특히 대규모 언어 모델(LLM)의 등장은 전통적 프로세스를 근본적으로 변화시키고 있다. ChatGPT, Claude 같은 도구들은 7단계를 하나의 대화형 인터페이스로 통합하여, 수개월 걸리던 프로세스를 수분으로 단축시킨다. 자연어 인터페이스를 통해 비기술 사용자도 복잡한 데이터 분석을 수행할 수 있고, 실시간 반복이 가능해지며, 확장성과 대중화가 실현된다.

### 데이터 제품 {#sec-data-product}

경로 A(의사결정)가 인사이트를 일회성 의사결정으로 전환한다면, 경로 B(데이터 제품)는 지속적 자동화 서비스로 전환한다. **데이터 제품**(Data Product)은 분석 결과를 프로덕션 시스템으로 구축하여 인간 개입 없이 지속적으로 가치를 창출하는 서비스다. 고객 이탈 모델을 예로 들면, 경로 A는 경영진이 VIP팀을 신설하는 의사결정을 내리고, 경로 B는 같은 모델을 API로 배포하여 매일 100만 고객을 자동 스코어링하고 CRM에 연동한다. 핵심 차이는 명확하다. 경로 A는 인간이 주도하고(경영진, 팀), 경로 B는 시스템이 주도한다(API, 자동화). 경로 A는 월/분기 단위로 반복되지만, 경로 B는 24시간 실시간으로 작동한다. 경로 A 확장성은 인력에 비례하지만(선형), 경로 B는 인프라에 비례한다(지수적). 성공적인 프로젝트는 두 경로를 병렬로 진행한다. API가 대규모 자동 스크리닝을 수행하고, VIP팀이 최고위험 고객을 집중 관리하여 인간과 시스템의 협업으로 가치를 극대화한다.

경로 B의 고도화가 MLOps/DataOps다. CRISP-DM 배포는 일회성 배포에 그치지만, MLOps는 지속적 운영을 다룬다. @fig-mlops-dataops 는 ML/데이터를 위한 지속적 통합 및 배포 파이프라인을 시각화한다. 중앙 MLOps는 자동화 및 CI/CD를 통해 전체 순환 프로세스를 조율한다. **데이터 수집** 단계에서 파이프라인과 ETL을 통해 원천 데이터를 확보하고, **피처 저장소** 단계에서 피처 엔지니어링으로 학습 준비를 마친다. **모델 학습** 단계에서는 AutoML과 실험을 통해 최적 모델을 생성하고, **배포** 단계에서 Kubernetes와 컨테이너로 프로덕션 환경에 적용한다. **모니터링** 단계에서는 관측성(Observability)을 통해 모델 성능과 시스템 상태를 추적하며, 다시 데이터 수집으로 순환한다. 주변의 인프라 구성 요소들(Git/GitOps, Docker, 클라우드, 테스팅, 메트릭)이 이 순환을 지원한다.

![MLOps / DataOps 파이프라인](images/ds-continuum-mlops-dataops.svg){#fig-mlops-dataops fig-align="center"}

**모델 라이프사이클 관리 핵심**은 자동화된 순환이다. MLflow 같은 모델 레지스트리는 모델 버전 관리와 A/B 테스트를 담당하고, GitHub Actions 같은 CI/CD 파이프라인은 코드 변경 시 자동 배포를 실행한다. KServe 같은 모델 서빙 도구는 고가용성과 Auto-scaling을 제공하며, Prometheus와 Grafana는 Latency와 Throughput을 추적한다. Evidently AI는 데이터/컨셉 드리프트를 자동 탐지하고, Airflow는 드리프트 감지 시 자동 재학습을 트리거한다. 예를 들어 이탈 예측 API는 1주차에 초기 배포(v1.0, XGBoost, 85% 정확도)되고, 3개월 후 드리프트 감지로 정확도가 79%로 하락하면, Airflow가 자동으로 최근 3개월 데이터로 재학습하고, Staging 환경에서 A/B 테스트를 실행하며, 성능 향상 확인(정확도 87%) 후 Production에 자동 배포(v1.1)한다. 결과적으로 인간 개입 없이 모델 품질이 유지된다. 주요 특징은 자동화 파이프라인, 코드형 인프라, 지속적 모니터링, 확장성이다.

## AI 지능 프레임워크 {#sec-beyond-dikw}

성공적인 데이터 과학 프로젝트는 경로 A(의사결정)와 경로 B(데이터 제품)를 모두 활용한다. 경로 A는 VIP팀 신설이나 프로세스 변경 같은 조직 변화에 집중하며, 인간의 전문성과 판단력을 활용하여 복잡한 의사결정과 윤리적 판단을 수행한다. 경로 B는 이탈 예측 API나 추천 엔진 같은 기술 시스템을 구축하여 자동화와 확장성을 확보하고, 반복적 작업과 대규모 처리를 담당한다. 두 경로의 시너지는 명확하다. API가 100만 고객을 대규모 자동 스크리닝하면, VIP팀은 최고위험 고객 500명에 집중 관리하여 인간과 시스템 협업으로 가치를 극대화한다. 대부분의 현대 데이터 과학 프로젝트는 두 경로를 병렬로 진행하며 상호보완적으로 가치를 창출한다.

DIKW 계층 구조는 1989년 이후 데이터 과학의 표준 프레임워크로 자리 잡았지만, 2025년 현재 AI 시대의 현실을 완전히 설명하기에는 한계가 있다. 전통적 DIKW는 인간 중심의 순차적 변환을 전제로 하지만, 대규모 언어 모델(LLM)과 멀티모달 AI는 근본적으로 다른 방식으로 작동한다.

![AI 지능 프레임워크: DIKW를 넘어선 새로운 패러다임](images/ds-continuum-beyond-dikw.svg){#fig-beyond-dikw fig-align="center"}

@fig-beyond-dikw 는 2025년 AI 시대에 맞춰 재정의된 지능 생성 프레임워크를 시각화한다. 전통적 DIKW의 4단계를 AI 중심으로 재해석한 새로운 계층 구조로, 중앙의 **기초 모델**(Foundation Models - LLM, 멀티모달 AI)을 중심으로 4개 계층이 순차적으로 변환되며 최종적으로 창발적 역량으로 귀결된다.

**좌측에서 우측으로의 변환 흐름**은 전통적 DIKW와 근본적으로 다른 방식으로 작동한다. 첫 번째 계층인 **원시 신호**(Raw Signals)는 구조화된 숫자와 기호를 전제했던 전통적 데이터 개념을 넘어, 비구조화 텍스트, 이미지, 오디오, 비디오 등 다양한 모달리티를 사전 정제 없이 직접 처리한다. 이는 전통적 데이터 파이프라인에서 필수였던 ETL 단계를 대폭 간소화한다. 두 번째 계층인 **임베딩**(Embeddings)은 원시 신호를 인코딩(Encoding) 과정을 거쳐 고차원 벡터 공간의 점으로 변환한다. 인간이 읽을 수 있는 텍스트나 차트가 아니라, 의미적 유사성을 수학적으로 계산 가능하게 만드는 벡터 표현이 핵심이며, Pinecone, Weaviate 같은 벡터 데이터베이스가 이 계층의 핵심 기술이다. 세 번째 계층인 **맥락**(Context)은 DIKW에는 없었던 완전히 새로운 계층이다. 임베딩을 증강(Augmentation) 과정을 거쳐 프롬프트 엔지니어링, Few-shot 학습, 도메인 지식을 결합하여, 대규모 언어 모델에게 적절한 맥락을 제공한다. 모델의 출력은 입력 프롬프트 구성 방식에 극도로 민감하며, RAG(Retrieval-Augmented Generation) 같은 기법이 이 단계에서 작동한다. 네 번째 계층인 **지능**(Intelligence)은 맥락을 생성(Generation) 과정을 거쳐 창발적 추론 능력으로 전환한다. 명시적으로 학습되고 저장된 정보가 아니라, 명시적으로 프로그래밍되지 않은 추론 체인을 생성하고, 학습 데이터에 없던 패턴을 합성하며, 완전히 새로운 콘텐츠를 창의적으로 생성한다. LangChain, LlamaIndex 같은 LLM 프레임워크가 이 계층을 구현한다.

**최종 계층인 창발적 역량**(Emergent Capabilities)은 전통적 지혜 개념이 인간의 가치 판단을 전제했던 것과 달리, 동적 적응, 지속적 학습, 에이전트 행동이라는 완전히 새로운 특성을 보인다. 모델은 지속적 피드백 및 강화를 통해 자율적으로 진화하며, AutoGen, CrewAI 같은 멀티 에이전트 오케스트레이션 도구가 이를 가능하게 한다. 우측 패널의 **패러다임 전환**은 이러한 변화의 본질을 명확히 보여준다. 저장에서 생성으로(데이터를 조회하는 것이 아니라 필요 시점에 생성), 쿼리에서 대화로(구조화된 쿼리 언어가 아니라 자연어 대화), 규칙에서 패턴으로(명시적 규칙이 아니라 데이터에서 학습한 패턴), 정적에서 적응적으로(고정된 시스템이 아니라 지속적 학습 시스템)의 4가지 전환이 동시에 일어난다. 중앙의 기초 모델은 이 모든 계층을 연결하는 핵심 엔진이며, GPT-4, Claude, Gemini 같은 대규모 언어 모델과 멀티모달 AI가 인간-AI 협업(Human-AI Collaboration)을 통해 전체 프레임워크를 완성한다.

DIKW는 여전히 유효한 개념적 프레임워크지만, AI 시대 데이터 과학자는 **두 프레임워크 모두를 이해**해야 한다. 전통적 DIKW는 "왜 데이터가 중요한가?"를 설명하고, 새로운 AI 프레임워크는 "어떻게 AI가 데이터를 처리하는가?"를 설명한다. 둘의 통합적 이해가 현대 데이터 과학의 핵심 역량이다.

## 프레임워크에서 실전으로

이 장에서 확인한 핵심은 명확하다. 데이터 과학은 단일 방법론이 아니라 CRISP-DM, TDSP, SEMMA 같은 구조화된 프로세스와 DIKW, AI 프레임워크 같은 개념적 계층이 결합된 체계이며, 2025년 현재 전통적 순차 프로세스에서 MLOps 기반 순환 파이프라인으로, 인간 중심 DIKW에서 AI 중심 생성 프레임워크로 근본적 전환이 일어나고 있다.

역사는 중요한 패턴을 보여준다. CRISP-DM이 1996년 등장한 이후 20년 넘게 표준으로 자리잡았지만, 2010년대 빅데이터와 클라우드의 확산은 TDSP 같은 엔터프라이즈 중심 방법론을 요구했고, 2020년대 AI 시대는 MLOps와 DataOps를 통한 지속적 배포 체계를 필수로 만들었다. 그리고 모든 기술 전환이 그랬듯, 진정한 혁신은 도구 자체가 아니라 **도구를 활용하는 프로세스의 재설계**에서 나온다. 전통적 7단계 연속체가 2-6개월 소요되던 의사결정을 MLOps는 일 단위 자동 재학습으로 압축했고, DIKW의 순차적 변환은 AI 프레임워크의 맥락 기반 생성으로 재정의되었다. 핵심은 프레임워크를 아는 것이 아니라 **상황에 맞는 프레임워크를 선택하고 조합하는 능력**이다.

현재 우리는 전통적 방법론의 안정성과 AI 기반 프레임워크의 유연성을 모두 요구받는 전환기에 서 있다. 경로 A(의사결정)와 경로 B(데이터 제품)를 병렬로 진행하고, DIKW와 AI 프레임워크를 통합적으로 이해하며, CRISP-DM의 구조와 MLOps의 자동화를 결합하는 것이 현대 데이터 과학자의 필수 역량이다. 다음 장부터는 이러한 프레임워크를 실제 데이터에 적용하여 인사이트를 도출하고 가치를 창출하는 구체적 기법을 다룬다.

