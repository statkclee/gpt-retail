# 데이터 {#sec-data}

AI가 데이터 과학 패러다임을 바꾸고 있다. 1부에서 ChatGPT가 범용기술로서 경제 전반에 미치는 영향과 AI 기술의 기본 원리를 살펴봤다면, 2부에서는 AI를 활용한 실전 데이터 과학 기술을 다룬다. 데이터 과학자가 AI와 협업하여 데이터를 수집하고, 정제하고, 분석하며, 인사이트를 도출하는 전체 워크플로우를 실습한다.

하지만 그 전에, 가장 기본적인 질문부터 시작해야 한다. **데이터란 무엇인가?** 질문은 단순해 보이지만, 과학, 컴퓨팅, 비즈니스, 철학 등 다양한 관점에서 서로 다른 답을 제시한다. 데이터 본질을 이해해야 AI와 효과적으로 협업할 수 있다.

## 데이터란 무엇인가

**데이터(Data)**는 라틴어 'datum'(주어진 것)에서 유래했다. 옥스퍼드 사전은 데이터를 "기록되고 처리될 수 있는 사실, 관찰, 또는 측정값"이라 정의한다. 하지만 간결한 정의 뒤에는 여러 학문 분야의 관점이 교차한다.

![다층적 데이터 정의](images/ds-what-is-data.svg){#fig-what-is-data}

@fig-what-is-data 는 데이터를 바라보는 여섯 가지 핵심 관점을 보여준다. 각 관점은 데이터의 서로 다른 측면을 강조하며, 모두 합쳐져 데이터의 전체 그림을 구성한다.

### 과학적 관점

**과학적 관점\index{과학적 관점}**에서 데이터는 **경험적 관찰(Empirical Observations)**의 결과다. 실험을 통해 수집된 측정값, 관찰 기록, 재현 가능한 증거가 데이터를 구성한다. 17세기 갈릴레오가 망원경으로 목성의 위성을 관찰하고 기록한 것, 19세기 멘델이 완두콩 교배 실험에서 얻은 형질 비율, 20세기 DNA 염기서열 분석 결과가 모두 과학적 데이터다.

과학에서 데이터 핵심 특성은 **재현성(Reproducibility)**이다. 같은 조건에서 실험을 반복하면 동일한 결과를 얻을 수 있어야 한다. 재현성은 과학적 지식의 신뢰성을 보장하는 기반이다. 데이터 과학자가 분석 코드를 공유하고 결과를 재현 가능하게 만드는 것도 이러한 과학적 전통의 연장선이다.

### 통계적 관점

**통계적 관점\index{통계적 관점}**은 데이터를 **변수(Variables)와 관측(Observations)**의 집합으로 본다. 각 행은 하나의 관측 단위(개인, 제품, 거래, 실험 등)를 나타내고, 각 열은 측정된 변수(나이, 가격, 날짜, 측정값 등)를 나타낸다. 통계학은 이러한 데이터에서 패턴을 발견하고, 분포를 분석하며, 모집단 특성을 추정한다.

통계적 데이터 핵심 개념은 **표본(Sample)**과 **모집단(Population)** 관계다. 수집한 데이터는 항상 전체의 일부이며, 추출된 표본을 통해 모집단에 대한 추론을 수행한다. 100명 고객 설문 조사 결과로 전체 고객층의 선호도를 추정하는 것이 대표적 예다. 데이터 과학에서 기계학습 모형을 만들 때, 훈련 데이터와 테스트 데이터를 분리하는 것도 이러한 통계적 사고에 기반한다.

### 컴퓨팅 관점

**컴퓨팅 관점\index{컴퓨팅 관점}**에서 모든 데이터는 궁극적으로 **0과 1의 조합**이다. 텍스트는 아스키(ASCII)나 UTF-8 인코딩으로, 이미지는 픽셀 RGB 값으로, 음성은 샘플링된 진폭 값으로 변환되어 컴퓨터 메모리에 저장된다. 아날로그 신호를 디지털로 변환하는 과정(Analog-to-Digital Conversion)은 현대 데이터 처리 기본이다.

컴퓨터 과학이 데이터에 기여한 핵심은 **저장과 전송의 효율성**이다. 압축 알고리즘은 데이터 크기를 줄이고, 암호화는 보안을 보장하며, 오류 정정 코드는 전송 중 손실을 복구한다. 데이터베이스 시스템은 수십억 개 레코드를 빠르게 검색하고, 분산 시스템은 전 세계에 걸쳐 데이터를 동기화한다. AI 모델이 대규모 데이터를 학습할 수 있는 것도 이러한 컴퓨팅 인프라 덕분이다.

### 비즈니스 관점

**비즈니스 관점\index{비즈니스 관점}**은 데이터를 **전략적 자산(Strategic Asset)**으로 본다. 고객 데이터는 타겟 마케팅을 가능하게 하고, 거래 데이터는 수요 예측을 지원하며, 운영 데이터는 효율성을 개선한다. "데이터는 21세기의 석유"라는 비유는 데이터가 현대 경제의 핵심 자원임을 강조한다.

비즈니스에서 데이터 가치는 **의사결정 개선**으로 측정된다. A/B 테스트로 웹사이트 전환율을 높이고, 추천 시스템으로 매출을 증대하며, 이탈 예측 모델로 고객 유지 비용을 절감한다. 데이터 기반 의사결정(Data-Driven Decision Making)은 직관이나 경험에 의존하는 것보다 일관되게 우수한 결과를 낸다.

수집만 하고 활용하지 않는 데이터는 비용일 뿐이기 때문에 데이터 비즈니스 가치가 자동으로 발형되지 않는다. **올바른 질문**을 던지고, **적절한 분석**을 수행하며, **실행 가능한 인사이트**를 도출하고 실행에 옮겨야만 가치가 실현된다. 

### 철학적 관점

**철학적 관점\index{철학적 관점}**은 데이터를 **지식의 원자재(Raw Material of Knowledge)**로 본다. 데이터 자체는 맥락 없이는 의미가 없다. "42"라는 숫자는 온도일 수도, 나이일 수도, 가격일 수도 있다. 데이터가 정보(Information)가 되려면 맥락이 필요하고, 정보가 지식(Knowledge)이 되려면 이해와 통합이 필요하며, 지식이 지혜(Wisdom)가 되려면 판단력이 필요하다.

이러한 위계를 **DIKW 피라미드\index{DIKW 피라미드}**(Data-Information-Knowledge-Wisdom)라 부른다. 데이터 과학자의 역할은 단순히 데이터를 수집하고 처리하는 것이 아니라, 데이터에서 정보를 추출하고, 정보를 지식으로 종합하며, 지식을 의사결정 지혜로 변환하는 것이다.

철학적으로 중요한 또 다른 질문은 **객관성(Objectivity)**이다. 데이터는 중립적 사실인가, 아니면 수집 과정에서 이미 편향을 담고 있는가? 측정 방법, 표본 선택, 분류 체계 모두 인간의 판단을 반영한다. AI 시대에는 알고리즘이 이러한 판단을 자동화하지만, 알고리즘 역시 설계자의 가정을 내포한다. 데이터 객관성을 비판적으로 검토하는 자세가 필수적인 이유다.

### 거버넌스와 윤리

**데이터 거버넌스와 윤리\index{데이터 거버넌스}**는 21세기의 새로운 화두다. 개인정보 보호, 동의 관리, 편향 탐지와 완화, 데이터 계보(Data Lineage) 추적, 책임성(Accountability) 확보가 핵심 과제다.

유럽 일반 데이터 보호 규정(GDPR)은 개인에게 본인 데이터에 대한 권리를 부여했고, 기업들은 데이터 수집 목적을 명확히 밝히고 동의를 얻어야 한다. 데이터 과학자는 법적 요구사항을 준수하면서도 분석 가치를 극대화하는 균형을 찾아야 한다.

**편향(Bias)**은 특히 AI 시대에 심각한 문제다. 과거 데이터에 내재된 사회적 편견이 AI 모델에 학습되면, 채용, 신용 평가, 의료 진단에서 차별적 결과를 낳는다. 아마존 채용 AI가 여성 지원자를 불리하게 평가했던 사례[@dastin2018amazon]는 데이터 편향의 위험을 보여준다. 데이터 과학자는 데이터 수집 단계부터 편향을 점검하고, 모델 결과의 공정성을 검증해야 한다.

**데이터 계보(Data Lineage)**는 데이터가 어디서 왔고, 어떻게 변환되었으며, 누가 접근했는지 추적하는 것이다. 규제 준수와 디버깅에 필수적이며, AI 모델의 설명 가능성(Explainability)을 확보하는 기반이다.

## 데이터 역사

데이터 역사는 인류 문명의 발전과 함께해왔다. 점토판에 새긴 거래 기록부터 현대 AI가 처리하는 대규모 데이터셋까지, 데이터는 인간이 지식을 기록하고 전달하며 의사결정을 내리는 핵심 도구였다. @fig-data-history 는 데이터의 진화를 4개 시대로 구분하여 보여준다. 각 시대는 기술 혁신과 함께 데이터의 형태, 규모, 활용 방식이 근본적으로 변화했음을 보여준다.

![데이터 역사적 진화](images/ds-data-history.svg){#fig-data-history}

### 디지털 이전 시대

데이터 역사는 **물리적 매체**에서 시작되었다. 기원전 3000년 메소포타미아 점토판에 새긴 쐐기 문자는 인류 최초의 체계적 기록이었다. 거래 내역, 세금 대장, 법률 문서가 점토판에 보존되었고, 수천 년이 지난 지금도 해독 가능하다. 기원전 2000년경 이집트에서는 **파피루스 두루마리**가 등장했다. 점토판보다 가볍고 휴대 가능한 파피루스는 데이터 이동성을 획기적으로 개선했다. 105년 중국 한나라 채륜은 **종이 제조법을 확립**하여 대량 생산 가능한 기록 매체를 만들었다. 종이는 이후 실크로드를 통해 서쪽으로 전파되며 지식 전달의 혁명을 일으켰다.

1450년 구텐베르크 **인쇄술**은 데이터 복제 비용을 극적으로 낮췄다. 손으로 필사하던 책이 기계로 대량 생산되면서 지식 민주화가 시작되었다. 18-19세기 산업혁명은 데이터 수요를 폭발시켰다. 인구 조사, 생산 기록, 재고 관리, 회계 장부가 기하급수적으로 증가했다. 1890년 미국 인구 조사국은 허먼 홀러리스(Herman Hollerith)가 개발한 **천공 카드(Punch Card)** 시스템을 도입하여 6,200만 명 인구 데이터를 기계적으로 집계했다. 수작업 대비 10분의 1 시간으로 처리를 완료한 이 혁신은 후에 IBM의 기반이 되었다. 비로소 데이터가 **기계 판독 가능**한 형태로 진화한 것이다.

1951년 UNIVAC I은 **자기 테이프 저장장치**를 최초로 상용화했다. 데이터가 물리적 매체에서 전자적 비트로 전환되기 시작하면서, 디지털 시대 여명을 밝혔다.

### 초기 디지털 시대

1956년 IBM 305 RAMAC는 5MB 용량 **하드 디스크 드라이브(HDD)**를 최초로 상용화했다. 50개 24인치 디스크로 구성된 이 장치는 혁명적이었다. 자기 테이프 순차 접근과 달리, 하드 디스크는 **임의 접근(Random Access)**이 가능했기 때문이다. 데이터 검색 속도가 극적으로 향상되면서 실시간 데이터 처리 기반을 마련했다. 1960년대 **CODASYL 데이터베이스**는 네트워크형 및 계층형 데이터 구조를 제공했다. 하지만 복잡한 포인터 관계 때문에 유지보수가 어려웠다. 1970년 코드 박사(E.F. Codd)가 제안한 **관계형 모델(Relational Model)**은 이러한 문제를 해결했다. 데이터를 테이블로 구조화하고, SQL(Structured Query Language)로 쿼리하는 방식은 직관적이고 강력했다.

1979년 Oracle 데이터베이스가 출시되며 **상용 RDBMS** 시대가 열렸다. IBM DB2(1983), Microsoft SQL Server(1989)가 뒤를 이었다. ACID(Atomicity, Consistency, Isolation, Durability) 트랜잭션 보장은 금융, 제조, 유통 산업에서 데이터 무결성을 확보하는 핵심이 되었다. 1980년대 **데이터 웨어하우징(Data Warehousing)**이 등장했다. OLAP(Online Analytical Processing), 스타 스키마(Star Schema)는 대규모 분석 쿼리를 효율적으로 처리했다.

1990년대는 **엔터프라이즈 시스템** 시대였다. ERP, CRM, BI 도구가 기업의 모든 부서 데이터를 통합했다. 하지만 데이터는 여전히 기업 내부에 고립되어 있었고, 대부분 정형 데이터였다.

### 빅데이터 시대

2000년대 인터넷 보급, 스마트폰 확산, 소셜미디어 등장은 데이터 패러다임을 근본적으로 바뿌면서 **3V 폭발**이 시작되었다. 규모(Volume)는 테라바이트에서 페타바이트로, 속도(Velocity)는 배치에서 실시간으로, 다양성(Variety)은 정형에서 비정형으로 확장되었다. 2003년 구글은 **GFS(Google File System)** 논문을 발표하며 **분산 저장** 아키텍처를 공개했다. 2004년 **MapReduce** 논문은 수천 대 서버에서 **병렬 처리**하는 방법을 제시했다. 2006년 아파치 하둡은 이를 오픈소스로 구현하여 누구나 페타바이트 데이터를 처리할 수 있게 했다. 페이스북, 야후, 링크드인은 하둡으로 사용자 행동 데이터를 분석하고 개인화 서비스를 제공했다.

2009년 **NoSQL 운동**이 시작되었다. MongoDB, Cassandra는 비정형 데이터와 수평 확장을 지원하며 관계형 DB 한계를 극복했다. 2012년 **클라우드 데이터 플랫폼**(AWS Redshift, Google BigQuery)이 등장하여 인프라 구축 없이 빅데이터 분석이 가능해졌다. 2014년 아파치 스파크는 **인메모리 처리**로 하둡보다 100배 빠른 속도를 제공했다. 실시간 스트리밍, Schema-on-read, 분산 컴퓨팅이 빅데이터 시대의 핵심 특성이 되었다.

데이터 과학자(Data Scientist)라는 직업이 등장한 것도 이 시기로, 2012년 하바드 비즈니스 리뷰는 "데이터 과학자는 21세기 가장 섹시한 직업"이라 선언하며 정점을 찍었다.

### AI 지능 시대

2017년 Google \"Attention Is All You Need\" 논문[@vaswani2017attention]은 **트랜스포머(Transformer) 아키텍처**를 제안했고, **LLM 기반**이 되었다. 2019년 **벡터 데이터베이스**(Weaviate 오픈소스, Pinecone 설립)가 등장하여 의미론적 검색을 가능하게 했다. 2020년 **GPT-3**는 1,750억 개 파라미터로 **대규모 언어 모델**의 가능성을 증명했다. 2022년 **ChatGPT 혁명**이 일어났다. 수억 명 사용자가 **대화형 AI**로 데이터를 분석하고 코드를 작성했다. 자연어가 프로그래밍 언어가 되었다. "지난 분기 매출 상위 10개 제품을 시각화해줘"라고 요청하면 AI가 데이터 추출, 분석, 시각화를 자동으로 수행한다.

2023년 **멀티모달 AI**(GPT-4, Gemini, Claude)는 텍스트, 이미지, 음성을 통합적으로 이해하고 생성했다. 데이터 과학자는 더 이상 데이터 유형별로 별도 파이프라인을 구축할 필요가 없다. 2024-25년 **에이전틱 AI(Agentic AI)**는 **자율 시스템**으로 진화하고 있다. AI가 목표를 설정하고, 데이터를 수집하며, 분석을 수행하고, 의사결정을 제안한다.

비정형 데이터 처리, 의미론적 이해, 실시간 지능, 자기학습 시스템이 AI 시대 핵심 특성이다. 데이터 과학자 역할은 점점 더 "AI와 협업하여 문제를 정의하고 결과를 검증하는 것"으로 전환되고 있다. **이러한 역사적 진화는 현대 데이터를 이해하는 새로운 분류 체계를 요구한다.**

## 데이터 특성

데이터 역사를 살펴본 결과, **디지털 이전 시대**는 물리적 매체와 순차 저장, **초기 디지털 시대**는 정형 데이터와 ACID 트랜잭션, **빅데이터 시대**는 3V와 분산 컴퓨팅, **AI 지능 시대**는 비정형 데이터와 의미론적 이해가 핵심이었다. 각 시대마다 데이터의 형태, 저장, 처리 방식이 근본적으로 변화했다.

![AI 시대 데이터 특성](images/ds-data-characteristics.svg){#fig-data-characteristics}

@fig-data-characteristics 는 이러한 진화를 반영하여 데이터를 분류하는 6가지 차원을 보여준다. 상단 3개(구조, 성격, 시간성)는 전통적 데이터 분류 체계이며, 하단 3개(양식, 출처, 관계)는 AI 시대에 새롭게 부각된 특성이다. 각 특성은 양극단을 가진 연속적 스펙트럼으로 이해할 수 있으며, 실제 데이터는 여러 차원 특성을 동시에 지닌다.

### 구조

**정형 데이터(Structured Data)\index{정형 데이터}**는 명확한 스키마를 가진 테이블 형태다. 각 열은 특정 데이터 타입(정수, 문자열, 날짜 등)을 가지며, 행은 독립적 관측을 나타낸다. 관계형 데이터베이스(PostgreSQL, SQLite3)에 저장되며, SQL로 쉽게 쿼리할 수 있다. 고객 정보, 거래 기록, 재고 데이터가 대표적이다.

**비정형 데이터(Unstructured Data)\index{비정형 데이터}**는 고정된 스키마가 없다. 텍스트 문서, 이미지, 동영상, 음성, 소셜미디어 게시물이 여기 해당한다. 전체 데이터 80-90%가 비정형이라 추정되지만, 전통적 도구로 분석하기 어렵다. 중간 형태로 **반정형 데이터(Semi-Structured Data)\index{반정형 데이터}**도 있는데, JSON, XML, 로그 파일이 대표적이다. 고정된 테이블 구조는 없지만 태그나 키-값 쌍으로 어느 정도 구조를 가진다.

AI는 비정형 데이터 분석을 혁신했다. 자연어 처리(NLP)로 텍스트를 분석하고, 컴퓨터 비전으로 이미지를 인식하며, 음성 인식으로 오디오를 텍스트로 변환한다. ChatGPT에게 "이 고객 리뷰들을 분석하여 긍정/부정/중립으로 분류하고 주요 불만 사항을 요약해줘"라고 요청하면, 비정형 텍스트에서 구조화된 인사이트를 즉시 추출한다.

### 성격

**정량 데이터(Quantitative Data)\index{정량 데이터}**는 숫자로 측정 가능하다. 연속형(키, 몸무게, 온도)과 이산형(고객 수, 클릭 수)으로 나뉜다. 산술 연산이 가능하며, 평균, 표준편차, 상관관계 같은 통계량을 계산할 수 있다. 정량 데이터는 시각화할 때 히스토그램, 산점도, 박스플롯을 주로 사용하며, 머신러닝에서는 회귀 모델의 목표 변수나 입력 피처로 활용된다. 예를 들어 "이번 달 매출 예측", "고객 평균 구매 금액 분석"은 모두 정량 데이터 분석이다.

**정성 데이터(Qualitative Data)\index{정성 데이터}**는 범주나 속성을 나타낸다. 명목형(성별, 지역, 제품 카테고리)과 순서형(만족도 등급, 교육 수준, 군대 계급)으로 구분된다. 빈도와 비율을 계산할 수 있지만, 평균은 의미가 없다. 정성 데이터는 막대 그래프, 파이 차트로 시각화하며, 머신러닝에서는 원-핫 인코딩(One-Hot Encoding)이나 레이블 인코딩으로 변환하여 사용한다. "고객 이탈 예측(이탈/유지)", "제품 카테고리 자동 분류"는 정성 데이터를 목표 변수로 하는 분류 문제다.

데이터 과학에서 두 유형을 적절히 혼합하여 사용한다. 회귀 분석은 정량 데이터를 예측하고, 분류 모델은 정성 데이터를 예측한다. 탐색적 데이터 분석(EDA)에서는 정성 변수를 그룹화 기준으로 사용하여 정량 변수의 분포를 비교한다. 예를 들어 "지역별 평균 매출"은 정성(지역)과 정량(매출)을 결합한 분석이다.

### 시간성 

**정적 데이터(Static Data)\index{정적 데이터}**는 시간에 따라 변하지 않는다. 역사적 거래 기록, 완료된 설문 조사, 과거 실험 결과가 여기 해당한다. 한 번 수집되면 고정되며, 재현 가능한 분석의 기반이 된다. 정적 데이터는 CSV, Parquet, 데이터베이스 스냅샷 같은 형태로 저장되며, 동일한 분석을 반복해도 같은 결과를 얻을 수 있다. 머신러닝 모델 학습에 사용되는 훈련 데이터셋은 대부분 정적 데이터로, 모델 성능을 비교하고 재현하는 데 필수적이다.

**동적 데이터(Dynamic Data)\index{동적 데이터}**는 실시간으로 생성되고 변화한다. 주식 시세, 센서 측정값, 소셜미디어 스트림, 웹 로그가 대표적이다. 스트리밍 데이터 처리(Apache Kafka, Flink)가 필요하며, 대시보드는 지속적으로 업데이트된다. IoT 센서 네트워크는 초당 수천 개의 측정값을 생성하고, 전자상거래 사이트는 실시간으로 클릭 스트림을 수집한다. 동적 데이터를 다룰 때는 데이터 지연(latency)과 처리량(throughput)을 고려해야 하며, 윈도우 함수를 사용하여 시간 구간별 집계를 수행한다.

AI 시대에는 동적 데이터 중요성이 커지고 있다. 실시간 추천 시스템은 사용자 최근 행동을 즉시 반영하고, 이상 탐지 시스템은 스트리밍 데이터에서 비정상 패턴을 즉각 감지한다. 하지만 동적 데이터는 재현성 확보가 어렵다는 과제가 있다. 데이터 스냅샷을 저장하거나 버전 관리를 통해 분석 결과의 재현 가능성을 유지해야 한다.

### 양식

**단일양식 데이터(Unimodal Data)\index{단일양식 데이터}**는 하나의 데이터 유형만 다룬다. 텍스트만 분석하는 감성 분석 모델, 이미지만 인식하는 객체 검출 시스템, 음성만 처리하는 STT(Speech-to-Text) 엔진이 대표적이다. 기계학습은 대부분 단일양식으로 각 데이터 유형마다 별도 모델과 파이프라인이 필요했다.

**다중양식 데이터(Multimodal Data)\index{다중양식 데이터}**는 텍스트, 이미지, 음성, 동영상을 통합적으로 이해한다. GPT-5는 이미지를 보고 설명하거나, 차트를 분석하여 인사이트를 제시한다. Google Gemini는 동영상 콘텐츠를 이해하고 요약한다. 이러한 다중양식 AI는 인간이 세상을 인식하는 방식과 유사하게 작동한다. 

실무에서 다중양식 AI는 강력한 도구다. 제품 사진과 설명을 함께 분석하여 카테고리를 자동 분류하고, 고객 리뷰 텍스트와 첨부 이미지를 통합 분석하여 불만 사항을 정확히 파악하며, 회의 동영상에서 음성과 화면을 함께 분석하여 핵심 내용을 요약한다. 데이터 과학자는 더 이상 데이터 유형별로 별도 모델을 구축할 필요가 없다.

### 출처

**실제 데이터(Real Data)\index{실제 데이터}**를 현실 세계 관찰, 측정, 수집으로 얻는다. 센서가 측정한 온도, 설문 조사 응답, 거래 기록, 의료 진단 결과가 모두 실제 데이터다. 전통적으로 데이터 과학은 실제 데이터에 크게 의존했다. 데이터가 부족하면 자원과 시간을 투입하여 추가로 데이터를 생산하거나 분석을 포기해야 했다.

**합성 데이터(Synthetic Data)\index{합성 데이터}**는 AI가 생성한 인공 데이터다. GAN(Generative Adversarial Networks)은 실제와 구별하기 어려운 이미지를 생성하고, Diffusion 모델은 텍스트 설명만으로 사진 같은 이미지를 만든다. 데이터 증강(Data Augmentation)도 합성의 한 형태로, 기존 이미지를 회전, 크롭, 색상 변경하여 학습 데이터를 늘린다.

합성 데이터는 세 가지 핵심 가치를 제공한다. 첫째, **프라이버시 보호**다. 의료 데이터, 금융 거래, 개인 정보는 규제로 공유가 어렵지만, 합성 데이터는 통계적 특성만 유지하고 개인 식별 정보를 제거하여 공유할 수 있다. 둘째, **데이터 부족 해결**이다. 희귀 질병 사례, 이상 거래 패턴처럼 실제 데이터가 부족한 경우 합성 데이터로 모델을 학습시킬 수 있다. 셋째, **비용 절감**이다. 자율주행 시뮬레이션, 로봇 훈련 환경을 합성으로 구축하면 실제 테스트보다 훨씬 저렴하게 학습할 수 있다.

하지만 합성 데이터 한계도 명확하다. 실제 세계의 복잡성과 예외 상황을 완벽히 재현하기 어렵고, 잘못 생성된 합성 데이터는 모델 편향을 오히려 악화시킬 수 있다. 특히 생성 모델이 학습한 데이터에 편향이 있었다면, 합성 데이터는 그 편향을 증폭시킬 위험이 있다. 또한 합성 데이터만으로 학습된 모델은 실제 환경에서 예상치 못한 극단적 사례나 예외 상황에 취약할 수 있다. 데이터 과학자는 실제 데이터와 합성 데이터를 적절히 혼합하여 활용하고, 합성 데이터 품질을 지속적으로 검증해야 한다.

### 관계

**독립 데이터(Independent Data)\index{독립 데이터}**는 각 행이 서로 독립적인 전통적 테이블 형태다. 고객 정보 테이블에서 각 고객은 다른 고객과 무관하고, 거래 기록에서 각 거래는 독립적 이벤트다. 통계 분석과 머신러닝의 대부분은 이러한 독립성 가정에 기반한다.

**그래프 데이터(Graph Data)\index{그래프 데이터}**는 노드와 엣지의 관계로 구조화된다. 소셜 네트워크에서 사용자는 노드이고 친구 관계는 엣지다. 지식 그래프\index{지식 그래프}는 개념 간 관계를 표현하며, \"서울\"-(수도)-\"대한민국\", \"삼성전자\"-(생산)-\"스마트폰\" 같은 구조를 가진다. 추천 시스템은 사용자-제품-카테고리의 복잡한 관계를 그래프로 모델링한다.

AI 시대에 그래프 데이터의 중요성이 급증하고 있다. **RAG(Retrieval-Augmented Generation)\index{RAG}**는 LLM 핵심 기술로 문서 간 의미적 관계를 그래프로 표현하고, 벡터 데이터베이스에서 관련 문서를 검색하여 AI 응답 정확도를 높인다. 검색 엔진은 웹페이지 간 링크 그래프를 분석하여 중요도를 계산하고, 사기 탐지 시스템은 거래 네트워크에서 의심스러운 패턴을 찾는다.

데이터 과학자는 이제 SQL만으로 충분하지 않다. Neo4j, Amazon Neptune 같은 그래프 데이터베이스, NetworkX, PyTorch Geometric 같은 그래프 분석 도구를 다뤄야 한다. 하지만 AI가 이러한 학습 곡선을 크게 완화한다. \"이 고객 네트워크에서 영향력이 큰 노드를 찾아줘\"라고 요청하면 AI가 적절한 중심성 알고리즘을 제안하고 코드를 생성한다.


### 품질

데이터 과학에는 "Garbage In, Garbage Out"\index{Garbage In, Garbage Out}이라는 원칙이 있다. 품질이 낮은 데이터로 아무리 정교한 모델을 만들어도 결과는 신뢰할 수 없다. 데이터 과학자는 실무 시간 대략 60-80%를 데이터 정제와 품질 검증에 사용한다. 데이터 품질은 여섯 가지 차원으로 평가된다.

**정확성(Accuracy)\index{정확성}**은 데이터가 실제를 올바르게 반영하는지 측정한다. 고객 주소가 잘못 입력되거나, 센서가 오류 값을 기록하면 분석 결과가 왜곡된다. **완전성(Completeness)\index{완전성}**은 필수 정보가 누락되지 않았는지 확인한다. 구매 기록에 결제 금액이 없거나, 설문 조사에서 필수 항목이 비어 있으면 분석이 불가능하다. **일관성(Consistency)\index{일관성}**은 동일한 데이터가 시스템 전체에서 일치하는지 검증한다. 고객 이름이 CRM에서는 "홍길동"인데 주문 시스템에서는 "Hong Jil Dong"으로 표기되면 통합 분석이 어렵다.

**적시성(Timeliness)\index{적시성}**은 데이터가 의사결정 시점에 유효한지 평가한다. 재고 관리에서 어제 데이터로 오늘 발주를 결정하면 품절이나 과잉 재고가 발생한다. **유효성(Validity)\index{유효성}**은 데이터가 정의된 형식과 규칙을 준수하는지 확인한다. 이메일 주소에 "@"가 없거나, 나이가 음수이거나, 날짜 형식이 잘못되면 시스템 오류가 발생한다. **고유성(Uniqueness)\index{고유성}**은 중복 데이터가 없는지 검증한다. 동일 고객이 중복 등록되면 매출 분석이 과대 계산되고, 중복 거래 기록은 재무 보고를 왜곡한다.

AI 시대에 데이터 품질 과제는 더욱 복잡해졌다. **합성 데이터 검증**이 추가로 필요하다. AI가 생성한 데이터가 실제 분포를 얼마나 정확히 재현하는지, 편향이 증폭되지 않았는지 지속적으로 모니터링해야 한다. **실시간 품질 관리**도 요구된다. 스트리밍 데이터 환경에서 이상 값을 즉시 탐지하고 제거해야 하며, 배치 처리처럼 사후 정제로는 부족하다. **다중양식 정합성**을 확인해야 한다. 제품 이미지와 설명이 일치하는지, 음성 데이터와 전사 텍스트가 정확한지 검증하는 작업이 추가되었다. AI는 품질 검증 자동화를 가속화하지만, 품질 기준 정의와 최종 검증은 여전히 데이터 과학자 책임이다.

## 데이터 생명주기

데이터는 생성되는 순간부터 삭제되는 시점까지 여러 단계를 거치는데 **데이터 생명주기(Data Lifecycle)\index{데이터 생명주기}**라 불리며, 각 단계마다 적절한 관리가 필요하다. 전통적으로 생명주기는 수집 → 저장 → 정제 → 변환 → 분석 → 공유 → 보관/폐기 7단계로 구성되었다. AI 시대에도 이러한 구조는 유지되지만, 각 단계에서 AI가 수행하는 역할이 근본적으로 달라졌다.

![데이터 생명주기](images/ds-data-lifecycle.svg){#fig-data-lifecycle}

**수집(Collection)\index{데이터 수집}** 단계에서는 데이터를 생성하거나 획득한다. 센서가 온도를 측정하고, 웹 스크래핑이 뉴스 기사를 수집하며, 고객이 주문 정보를 입력한다. 전통적으로 데이터 수집은 미리 정의된 스키마에 맞춰 정형 데이터를 수집했다. AI 시대에는 비정형 데이터 수집이 폭발적으로 증가했다. 이미지, 동영상, 음성, 텍스트를 대규모로 수집하고, AI는 웹 스크래핑 코드 작성, API 호출 자동화, 데이터 수집 파이프라인 설계를 지원한다.

**저장(Storage)\index{데이터 저장}** 단계에서는 수집한 데이터를 영구 보관한다. 관계형 데이터베이스(RDBMS), NoSQL, 데이터 레이크, 클라우드 스토리지가 대표적이다. 전통적으로 저장은 구조화된 데이터베이스에 정형 데이터를 저장했다. AI 시대에는 새롭게 **벡터 데이터베이스(Vector Database)\index{벡터 데이터베이스}**가 등장했다. 텍스트, 이미지, 음성을 임베딩 벡터로 변환하여 저장하고, 의미적 유사도 검색을 지원한다. Pinecone, Weaviate, ChromaDB가 RAG 시스템 핵심 인프라로 자리 잡았다. 데이터 과학자는 "데이터셋에 적합한 스토리지 솔루션을 추천해줘"라고 AI에 요청하면 데이터 특성, 쿼리 패턴, 비용을 고려한 제안을 받는다.

**정제(Cleaning)\index{데이터 정제}** 단계에서는 데이터 품질을 개선한다. 결측값을 처리하고, 이상 값을 제거하며, 중복을 제거하고, 형식을 표준화한다. 데이터 과학자는 실무 시간의 대부분을 정제 단계에서 소비한다. 전통적으로 정제는 수작업과 규칙 기반 스크립트로 수행했지만, AI 시대에는 정제 자동화가 가속화되었다. AI는 "데이터프레임에서 결측값을 적절한 방법으로 처리하는 코드를 작성해줘"라고 요청하면 컬럼 특성을 분석하여 평균 대체, 중앙값 대체, 예측 모델 기반 대체 중 적절한 방법을 제안한다. 자연어로 데이터 품질 이슈를 설명하면 정규표현식, 통계적 이상 탐지, 머신러닝 기반 정제 코드를 생성한다.

**변환(Transformation)\index{데이터 변환}** 단계에서는 분석 목적에 맞게 데이터를 재구조화한다. 정규화, 집계, 피벗, 조인, 파생 변수 생성이 포함된다. 전통적으로 변환은 SQL, ETL(Extract-Transform-Load) 도구, 파이썬 판다스로 수행했지만, AI 시대에는 변환 로직을 자연어로 표현할 수 있다. "판매 데이터를 월별 매출로 집계하고, 전월 대비 증감율을 계산해줘"라고 요청하면 AI가 적절한 판다스 코드를 생성한다. 복잡한 비즈니스 규칙도 자연어로 설명하면 코드로 변환된다.

**분석(Analysis)\index{데이터 분석}** 단계에서는 데이터로부터 인사이트를 추출한다. 통계 분석, 시각화, 머신러닝 모델링, 예측이 수행된다. 전통적으로 분석은 데이터 과학자 전문 영역으로 통계학, 머신러닝, 프로그래밍 지식이 필수였지만, AI 시대에는 분석 진입 장벽이 크게 낮아졌다. "고객 데이터에서 이탈 가능성이 높은 그룹을 찾아줘"라고 요청하면 AI가 탐색적 데이터 분석(EDA), 군집화, 분류 모델을 제안하고 코드를 생성한다. 비전문가도 AI와 협업하여 고급 분석을 수행할 수 있다.

**공유(Sharing)\index{데이터 공유}** 단계에서는 분석 결과를 이해관계자에게 전달한다. 보고서, 대시보드, 시각화, API가 대표적이다. 전통적으로 공유는 정적 문서나 수동 업데이트 대시보드로 이루어졌지만, AI 시대에는 인터랙티브 공유가 증가했다. Streamlit, Gradio로 AI 기반 웹 앱을 빠르게 구축하여 비전문가가 데이터를 탐색할 수 있다. AI는 "분석 결과를 경영진에게 보고할 시각화를 만들어줘"라고 요청하면 적절한 차트 유형을 제안하고 코드를 생성한다.

**보관/폐기(Archive/Disposal)\index{데이터 보관}** 단계에서는 데이터를 장기 보관하거나 안전하게 삭제한다. 법적 요구사항(GDPR, 개인정보보호법), 비용 최적화, 보안이 고려된다. 전통적으로 보관은 테이프 백업, 콜드 스토리지로 수행되었지만, AI 시대에는 **데이터 거버넌스\index{데이터 거버넌스}** 중요성이 급증했다. 개인 식별 정보(PII) 자동 탐지, 민감 데이터 마스킹, 삭제 정책 자동화가 필요하다. AI는 "데이터셋에서 개인정보를 자동으로 탐지하고 익명화하는 코드를 작성해줘"라고 요청하면 정규표현식, 개체명 추출(NER, Named Entity Recognition) 모델을 활용한 코드를 생성한다.

AI는 데이터 생명주기 전체를 변화시키고 있다. 수집부터 폐기까지 모든 단계에서 자동화, 지능화, 대중화가 진행된다. 중요한 것은 AI가 생명주기를 **대체**하는 것이 아니라 **가속**한다는 점이다. 데이터 과학자는 여전히 각 단계의 품질을 검증하고, 비즈니스 맥락을 이해하며, 윤리적 이슈를 관리해야 한다. AI는 반복 작업을 자동화하여 데이터 과학자가 더 높은 수준의 문제 해결에 집중할 수 있도록 지원한다.

## 데이터 재발견

이 장에서 확인한 핵심은 명확하다. 데이터는 단순한 숫자 모음이 아니라 **다차원적 실체**이며, 정의(무엇인가), 역사(어떻게 진화했는가), 특성(어떻게 분류하는가)을 통합적으로 이해해야 AI 시대의 데이터 과학이 가능하다는 것이다. 특히 AI 시대는 전통적 분류(구조, 성질, 양식)를 넘어 시간성, 출처, 관계라는 새로운 차원을 요구한다.

역사는 중요한 패턴을 보여준다. 기원전 3000년 점토판부터 2020년대 벡터 데이터베이스까지, 데이터 형태는 매체 기술에 종속되었다. 종이 시대는 순차 저장, 자기 테이프 시대는 배치 처리, 관계형 데이터베이스 시대는 ACID 트랜잭션, 빅데이터 시대는 분산 컴퓨팅이 핵심이었다. AI 시대는 **의미론적 이해**가 핵심이다. 과거 데이터는 "저장하고 검색"하는 대상이었다면, 현재 데이터는 "이해하고 생성"하는 대상이다. 데이터 품질은 더 이상 결측값 처리 수준이 아니라 합성 데이터 검증, 실시간 이상 탐지, 다중양식 정합성 확인을 포함한다. 데이터 생명주기는 수집부터 폐기까지 AI 지원을 받지만, 최종 책임은 여전히 데이터 과학자에게 있다.

현재 데이터를 **다루는 방법**을 넘어 **AI와 협업하는 방법**을 배워야 하는 전환점에 서 있다. 중요한 것은 데이터 이론적 분류를 아는 것이 아니라 실제 문제에 적용하는 능력이다. 다음 장부터는 AI 코딩 기술을 본격적으로 다룬다. ChatGPT, Claude, GitHub Copilot 같은 도구로 데이터를 수집하고, 정제하고, 분석하는 실전 기술을 익힌다.
