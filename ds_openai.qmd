# OpenAI API\index{OpenAI API}

OpenAI API\index{OpenAI API}ëŠ” ìµœì‹  ì¸ê³µì§€ëŠ¥ ëª¨ë¸ë“¤ì„ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë¥¼ í†µí•´ í™œìš©í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” ê°•ë ¥í•œ ë„êµ¬ë‹¤. GPT-4o\index{GPT-4o}, DALLÂ·E\index{DALLÂ·E} 3, Whisper\index{Whisper} ë“±ì˜ ì²¨ë‹¨ ëª¨ë¸ì— ì ‘ê·¼í•˜ì—¬ í…ìŠ¤íŠ¸ ìƒì„±, ì´ë¯¸ì§€ ìƒì„±, ìŒì„± ì²˜ë¦¬, ì„ë² ë”© ìƒì„± ë“± ë‹¤ì–‘í•œ AI ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤. OpenAI APIì˜ ì „ì²´ êµ¬ì¡°ì™€ ì£¼ìš” ê¸°ëŠ¥ë“¤ì´ @fig-openai-diagram ì œì‹œë˜ì–´ ìˆë‹¤.

```{mermaid}
%%| label: fig-openai-diagram
%%| fig-cap: "OpenAI API ì „ì²´ êµ¬ì¡°ì™€ í™œìš© ë°©ì•ˆ"
%%| fig-width: 4
graph LR
    subgraph Setup ["ğŸ”§ ì„¤ì •"]
        User[ğŸ‘¤ ê°œë°œì]
        Client[ğŸ“± OpenAI<br/>í´ë¼ì´ì–¸íŠ¸]
        User --> Client
    end
    
    subgraph Core ["ğŸ¯ í•µì‹¬ API"]
        direction TB
        
        subgraph Text ["ğŸ’¬ í…ìŠ¤íŠ¸"]
            Chat[Chat API]
            Models1["GPT-4o/mini<br/>o1 ì‹œë¦¬ì¦ˆ"]
            Chat --- Models1
        end
        
        subgraph Media ["ğŸ¨ ë¯¸ë””ì–´"]
            Audio[ğŸµ ìŒì„±]
            Image[ğŸ–¼ï¸ ì´ë¯¸ì§€]
            Models2["Whisper-1<br/>DALLÂ·E 3/2"]
            Audio --- Models2
            Image --- Models2
        end
        
        subgraph Data ["ğŸ“Š ë°ì´í„°"]
            Embed[ğŸ”¢ ì„ë² ë”©]
            Mod[ğŸ›¡ï¸ ì¤‘ì¬]
            Models3["embedding-3<br/>moderation"]
            Embed --- Models3
            Mod --- Models3
        end
    end
    
    subgraph Apps ["ğŸ¯ í™œìš©"]
        direction TB
        
        subgraph Business ["ğŸ’¼ ë¹„ì¦ˆë‹ˆìŠ¤"]
            Service[ğŸ¤– ê³ ê°ì„œë¹„ìŠ¤]
            Analysis[ğŸ“ˆ ë¶„ì„]
        end
        
        subgraph Tech ["âš™ï¸ ê°œë°œ"]
            Code[ğŸ’» ì½”ë”©]
            Search[ğŸ” ê²€ìƒ‰]
        end
        
        subgraph Safe ["ğŸ”’ ì•ˆì „"]
            Content[ğŸ›¡ï¸ ê´€ë¦¬]
            Policy[ğŸ“‹ ì •ì±…]
        end
    end
    
    subgraph Opt ["ğŸ’¡ ìµœì í™”"]
        direction LR
        Strategy[ğŸ“Š ì „ëµ]
        Cost[ğŸ’° ë¹„ìš©]
        Monitor[ğŸ“ˆ ëª¨ë‹ˆí„°ë§]
        
        Strategy --> Cost
        Cost --> Monitor
    end
    
    %% ì—°ê²°
    Client --> Core
    Text --> Business
    Text --> Tech
    Media --> Business
    Data --> Safe
    Core --> Apps
    Client --> Opt
    
    %% ìŠ¤íƒ€ì¼
    classDef setupStyle fill:#fce4ec,stroke:#c2185b,stroke-width:2px
    classDef coreStyle fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef appStyle fill:#f1f8e9,stroke:#689f38,stroke-width:2px
    classDef optStyle fill:#fff8e1,stroke:#f57c00,stroke-width:2px
    
    class Setup,User,Client setupStyle
    class Core,Text,Media,Data,Chat,Audio,Image,Embed,Mod coreStyle
    class Apps,Business,Tech,Safe appStyle
    class Opt,Strategy,Cost,Monitor optStyle
```

API ì‚¬ìš© ì‹œ ë³´ì•ˆì´ ê°€ì¥ ì¤‘ìš”í•œ ê³ ë ¤ì‚¬í•­ì´ë‹¤. `.env` íŒŒì¼ì— OpenAI API-KEYë¥¼ ì €ì¥í•œ ê²½ìš° `.gitignore`ì— `.env`ë¥¼ ê¸°ë¡í•˜ì—¬ í˜‘ì—…ê³¼ ê³µê°œë¥¼ í•  ê²½ìš° ì£¼ìš” ì •ë³´ê°€ ì™¸ë¶€ì— ë…¸ì¶œë˜ì§€ ì•Šë„ë¡ ì£¼ì˜í•œë‹¤.

## `openai` íŒ¨í‚¤ì§€

OpenAIì˜ íŒŒì´ì¬ í´ë¼ì´ì–¸íŠ¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” API í˜¸ì¶œì„ ê°„í¸í•˜ê²Œ ë§Œë“¤ì–´ì£¼ëŠ” í•„ìˆ˜ ë„êµ¬ë‹¤. ì´ íŒ¨í‚¤ì§€ëŠ” ë³µì¡í•œ HTTP ìš”ì²­ì„ ê°„ë‹¨í•œ ë©”ì„œë“œ í˜¸ì¶œë¡œ ë³€í™˜í•´ì£¼ë©°, íƒ€ì… íŒíŠ¸ì™€ ìë™ì™„ì„±ì„ ì§€ì›í•˜ì—¬ ê°œë°œ ìƒì‚°ì„±ì„ í¬ê²Œ í–¥ìƒì‹œí‚¨ë‹¤. GitHub ì €ì¥ì†Œ [OpenAI Python Library](https://github.com/openai/openai-python) [@openai2025python]ì˜ `openai` íŒŒì´ì¬ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•œ í›„ ë²„ì „ì„ í™•ì¸í•œë‹¤.

```{python}
#| eval: false
! pip install openai

# ë²„ì „ í™•ì¸
! pip show openai
```

## ê¸°ë³¸ ì„¤ì •

API ì‚¬ìš© ì „ í´ë¼ì´ì–¸íŠ¸ë¥¼ ì ì ˆíˆ ì´ˆê¸°í™”í•˜ëŠ” ê²ƒì´ ëª¨ë“  ì‘ì—…ì˜ ì¶œë°œì ì´ë‹¤. í™˜ê²½ë³€ìˆ˜ë¥¼ í†µí•œ API í‚¤ ê´€ë¦¬ëŠ” ë³´ì•ˆê³¼ í¸ì˜ì„±ì„ ë™ì‹œì— í™•ë³´í•˜ëŠ” ì—…ê³„ í‘œì¤€ ë°©ë²•ì´ë‹¤. OpenAI APIë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•œ ê¸°ë³¸ ì„¤ì •ì€ ë‹¤ìŒê³¼ ê°™ë‹¤:

```{python}
#| eval: false
from openai import OpenAI
from dotenv import load_dotenv
import os

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = OpenAI(
    api_key=os.getenv('OPENAI_API_KEY')
)
```

## í…ìŠ¤íŠ¸ API

í…ìŠ¤íŠ¸(Chat Completions) APIëŠ” OpenAIì˜ í•µì‹¬ ê¸°ëŠ¥ìœ¼ë¡œ, ëŒ€í™”í˜• AIë¥¼ êµ¬í˜„í•˜ëŠ” ê°€ì¥ ì¼ë°˜ì ì¸ ë°©ë²•ì´ë‹¤. ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¥¼ í†µí•´ AIì˜ ì—­í• ê³¼ ì„±ê²©ì„ ì •ì˜í•˜ê³ , ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ í†µí•´ ì‹¤ì œ ì§ˆë¬¸ì´ë‚˜ ìš”ì²­ì„ ì „ë‹¬í•  ìˆ˜ ìˆë‹¤. ì˜¨ë„(temperature) ì„¤ì •ìœ¼ë¡œ ì‘ë‹µì˜ ì°½ì˜ì„±ì„ ì¡°ì ˆí•˜ë©°, ìµœëŒ€ í† í° ìˆ˜ë¡œ ì‘ë‹µ ê¸¸ì´ë¥¼ ì œí•œí•  ìˆ˜ ìˆë‹¤. ìµœì‹  OpenAI APIëŠ” `client.chat.completions.create()` ë©”ì„œë“œë¥¼ ì‚¬ìš©í•œë‹¤.

```{python}
#| eval: false
response = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[
        {"role": "system", "content": "ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ë¡œ ëŒ€ë‹µë„ ê°„ê²°í•˜ê²Œ í•´"},
        {"role": "user", "content": "íŒŒì´ì¬ê³¼ R ì¤‘ ì–´ë–¤ ì–¸ì–´ê°€ ë°ì´í„° ë¶„ì„ì— ë” ì í•©í•œê°€ìš”?"}
    ],
    max_tokens=100,
    temperature=0
)

print(response.choices[0].message.content)
```

``` markdown
íŒŒì´ì¬ê³¼ R ëª¨ë‘ ë°ì´í„° ë¶„ì„ì— ì í•©í•˜ì§€ë§Œ, ê°ê°ì˜ ì¥ë‹¨ì ì´ ìˆìŠµë‹ˆë‹¤.

- **íŒŒì´ì¬**: 
  - ë²”ìš© í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë¡œ, ë°ì´í„° ë¶„ì„ ì™¸ì—ë„ ì›¹ ê°œë°œ, ìë™í™” ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ì‚¬ìš©.
  - Pandas, NumPy, Matplotlib, Seaborn ë“± ê°•ë ¥í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì§€ì›.
  - ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì— ê°•ì  (Scikit-learn,
```

::: callout-note
### ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸

ì‘ì—…ì˜ ë³µì¡ì„±ê³¼ ì˜ˆì‚°ì— ë”°ë¼ ì ì ˆí•œ ëª¨ë¸ì„ ì„ íƒí•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤. ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ì²˜ë¦¬ì—ëŠ” gpt-4o-miniê°€ íš¨ìœ¨ì ì´ë©°, ë³µì¡í•œ ì¶”ë¡ ì´ë‚˜ ë©€í‹°ëª¨ë‹¬ ì‘ì—…ì—ëŠ” gpt-4oë‚˜ o1 ì‹œë¦¬ì¦ˆê°€ ì í•©í•˜ë‹¤.

-   `gpt-4o`: ê°€ì¥ ê°•ë ¥í•œ ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ (ì´ë¯¸ì§€, í…ìŠ¤íŠ¸ ì²˜ë¦¬)
-   `gpt-4o-mini`: GPT-4oì˜ ê²½ëŸ‰í™” ë²„ì „, ë¹ ë¥´ê³  ì €ë ´
-   `gpt-4-turbo`: ìµœì‹  GPT-4 Turbo ëª¨ë¸ (128K ì»¨í…ìŠ¤íŠ¸)
-   `gpt-4`: í‘œì¤€ GPT-4 ëª¨ë¸
-   `gpt-3.5-turbo\index{gpt-3.5-turbo}`: ë¹ ë¥´ê³  ì €ë ´í•œ ëª¨ë¸
-   `o1-preview`: ë³µì¡í•œ ì¶”ë¡  ì‘ì—…ì— íŠ¹í™”ëœ ëª¨ë¸
-   `o1-mini`: o1ì˜ ê²½ëŸ‰í™” ë²„ì „

:::

## ìŒì„± ì²˜ë¦¬

Whisper APIë¥¼ í†µí•œ ìŒì„± ì²˜ë¦¬ëŠ” ë‹¤êµ­ì–´ ìŒì„± ì¸ì‹ê³¼ ë²ˆì—­ì„ ë™ì‹œì— ì§€ì›í•˜ëŠ” ê°•ë ¥í•œ ê¸°ëŠ¥ì´ë‹¤. ê¸°ì¡´ì˜ ìŒì„± ì¸ì‹ ì†”ë£¨ì…˜ë³´ë‹¤ ì •í™•ë„ê°€ ë†’ìœ¼ë©°, íŠ¹íˆ í•œêµ­ì–´ì™€ ê°™ì€ ë¹„ì˜ì–´ê¶Œ ì–¸ì–´ì—ì„œë„ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤. ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ê±°ë‚˜ ë‹¤ë¥¸ ì–¸ì–´ë¡œ ë²ˆì—­í•˜ëŠ” ì‘ì—…ì„ ê°„ë‹¨í•œ API í˜¸ì¶œë¡œ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤.


::: callout-note
### ì‚¬ìš© ê°€ëŠ¥í•œ ìŒì„± ëª¨ë¸

OpenAI APIë¥¼ í†µí•´ í˜„ì¬ ë‹¤ìŒ ëª¨ë¸ì„ ì œê³µí•˜ê³  ìˆë‹¤.

-   `whisper-1`: OpenAI APIì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ í‘œì¤€ Whisper ëª¨ë¸ (large-v2 ê¸°ë°˜)
-   ê°€ê²©: \$0.006/ë¶„
-   ë‹¤êµ­ì–´ ì§€ì›
-   ìŒì„± ì¸ì‹ ë° ë²ˆì—­ ì§€ì›

:::


### ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜

ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” STT(Speech-to-Text) ê¸°ëŠ¥ì€ íŒŸìºìŠ¤íŠ¸ ìë§‰ ìƒì„±, íšŒì˜ë¡ ì‘ì„±, ìŒì„± ëª…ë ¹ ì²˜ë¦¬ ë“± ë‹¤ì–‘í•œ ìš©ë„ë¡œ í™œìš©ëœë‹¤. Whisper ëª¨ë¸ì€ ë°°ê²½ ì†ŒìŒì´ ìˆëŠ” í™˜ê²½ì—ì„œë„ ë†’ì€ ì •í™•ë„ë¥¼ ìœ ì§€í•˜ë©°, í•œêµ­ì–´ì˜ ë³µì¡í•œ ìŒì„± ë³€í™”ë„ ì˜ ì²˜ë¦¬í•œë‹¤. [AI Hub](https://www.aihub.or.kr/) ì›¹ì‚¬ì´íŠ¸ì—ì„œ [í•œêµ­ì–´ ìŒì„±](https://www.aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=123) [@aihub2025korean] ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œ ë°›ì•„ STTë¥¼ ì‹¤ìŠµí•œë‹¤.

```{r}
#| eval: true
library(av)
library(embedr)

embedr::embed_audio("data/KsponSpeech_025980.mp3")
```

```{python}
#| eval: false
# ìŒì„± íŒŒì¼ ì—´ê¸°
with open("data/KsponSpeech_025980.mp3", "rb") as audio_file:
    response = client.audio.transcriptions.create(
        model="whisper-1",
        file=audio_file,
        language="ko",  # í•œêµ­ì–´ ì§€ì •
        response_format="text"  # text, json, srt, verbose_json, vtt ì¤‘ ì„ íƒ
    )

# ë³€í™˜ëœ í…ìŠ¤íŠ¸ ì €ì¥
with open("data/KsponSpeech_025980.txt", "w", encoding="utf-8") as f:
    f.write(response)
```

::: {layout-ncol="2"}
#### TTS ì¶œë ¥ê²°ê³¼ {.unnumbered}

``` markdown
'ì˜¤íˆë ¤ ë‚´ê°€ ë˜ ì•ˆì˜¬ë¼ê°„ ì´ìœ  ì¤‘ì— í•˜ë‚˜ê°€\n'
```

#### ì›ë³¸ {.unnumbered}

``` markdown
ì˜¤íˆë ¤ ë‚´ê°€ ë˜ ì•ˆì˜¬ë¼ê°„ ì´ìœ  ì¤‘ì— í•˜ë‚˜ê°€\n
```
:::

### ìŒì„± ë²ˆì—­

ìŒì„± ë²ˆì—­ ê¸°ëŠ¥ì€ ìŒì„± ì¸ì‹ê³¼ ë²ˆì—­ì„ í•œ ë²ˆì— ì²˜ë¦¬í•˜ëŠ” íš¨ìœ¨ì ì¸ ì†”ë£¨ì…˜ì´ë‹¤. ë‹¤êµ­ì–´ íšŒì˜ë‚˜ êµ­ì œ ê°•ì—°ì—ì„œ ì‹¤ì‹œê°„ ë²ˆì—­ì´ í•„ìš”í•  ë•Œ ìœ ìš©í•˜ë©°, ì›ë³¸ ì–¸ì–´ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•œ í›„ ë²ˆì—­í•˜ëŠ” 2ë‹¨ê³„ ê³¼ì •ì„ 1ë‹¨ê³„ë¡œ ë‹¨ì¶•ì‹œí‚¨ë‹¤. Whisper API [@openai2025whisper]ëŠ” ë‹¤ì–‘í•œ ì–¸ì–´ì˜ ìŒì„±ì„ ì˜ì–´ë¡œ ë²ˆì—­í•  ìˆ˜ ìˆë‹¤:

```{python}
#| eval: false
with open("data/á„Œá…¦84á„Œá…®á„‚á…§á†«_31á„Œá…¥á†¯_á„€á…µá„‚á…§á†·á„‰á…¡_á„‚á…©á„†á…®á„’á…§á†«.mp3", "rb") as audio_file:
    response = client.audio.translations.create(
        model="whisper-1",
        file=audio_file,
        prompt="í•œêµ­ ëŒ€í†µë ¹ì˜ ì—°ì„¤ì…ë‹ˆë‹¤."  # ì»¨í…ìŠ¤íŠ¸ ì œê³µìœ¼ë¡œ ì •í™•ë„ í–¥ìƒ
    )

print(response.text)
```

``` markdown
Honorable citizens, On the 84th anniversary of the Korean War, I express my deep gratitude and respect to the patriots who sacrificed for the country. 

... ì¤‘ëµ

Let us open the East-West era of peace and prosperity through unification and reform. Let us pass on the proud Republic of Korea to our descendants. Thank you. Thank you.
```

## ì´ë¯¸ì§€ ìƒì„±

í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¡œë¶€í„° ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” DALLÂ·E APIëŠ” ì°½ì‘ í™œë™ì˜ ìƒˆë¡œìš´ ì§€í‰ì„ ì—´ì–´ì£¼ëŠ” í˜ì‹ ì ì¸ ë„êµ¬ë‹¤. ë””ìì¸ ì‹œì•ˆ ì‘ì„±, ì½˜í…ì¸  ì œì‘, êµìœ¡ ìë£Œ ê°œë°œ ë“±ì—ì„œ ì‹œê°„ê³¼ ë¹„ìš©ì„ í¬ê²Œ ì ˆì•½í•  ìˆ˜ ìˆë‹¤. íŠ¹íˆ DALLÂ·E 3ëŠ” í”„ë¡¬í”„íŠ¸ë¥¼ ìë™ìœ¼ë¡œ ìµœì í™”í•˜ì—¬ ì‚¬ìš©ìì˜ ì˜ë„ë¥¼ ë” ì •í™•í•˜ê²Œ ë°˜ì˜í•œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•œë‹¤. DALLÂ·E API [@openai2025dalle]ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¡œë¶€í„° ê³ í’ˆì§ˆ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•  ìˆ˜ ìˆë‹¤.

::: callout-note

### ì‚¬ìš© ê°€ëŠ¥í•œ ì´ë¯¸ì§€ ëª¨ë¸

-   `dall-e-3`: ìµœì‹  ì´ë¯¸ì§€ ìƒì„± ëª¨ë¸ (ê³ í’ˆì§ˆ, í”„ë¡¬í”„íŠ¸ ìµœì í™” í¬í•¨)
-   `dall-e-2`: ì´ì „ ë²„ì „ (ë” ë¹ ë¥´ê³  ì €ë ´)
-   `gpt-4o`: ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ (í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ í†µí•© ì´í•´, ì œí•œì  ì ‘ê·¼)

:::

### ê¸°ë³¸ ì´ë¯¸ì§€ ìƒì„±

ì´ë¯¸ì§€ ìƒì„± ì‹œ í”„ë¡¬í”„íŠ¸ì˜ êµ¬ì²´ì„±ê³¼ ëª…í™•ì„±ì´ ê²°ê³¼ë¬¼ì˜ í’ˆì§ˆì„ í¬ê²Œ ì¢Œìš°í•œë‹¤. ìŠ¤íƒ€ì¼, ìƒ‰ìƒ, êµ¬ë„, ë¶„ìœ„ê¸° ë“±ì„ êµ¬ì²´ì ìœ¼ë¡œ ëª…ì‹œí• ìˆ˜ë¡ ì›í•˜ëŠ” ê²°ê³¼ë¥¼ ì–»ì„ ê°€ëŠ¥ì„±ì´ ë†’ì•„ì§„ë‹¤.

```{python}
#| eval: false
response = client.images.generate(
    model="dall-e-3",  # ë˜ëŠ” "dall-e-2"
    prompt="ì„œìš¸ ê°•ë‚¨ ê±°ë¦¬, ì—¬ë¦„ë‚ , ë°ê³  ì•„ë¦„ë‹¤ìš´ í’ê²½, ì˜í™”ì ì¸ ì¥ë©´ ì§€ë¸Œë¦¬ ìŠ¤íƒ€ì¼",
    size="1024x1024",  # DALL-E 3: 1024x1024, 1792x1024, 1024x1792
    quality="standard",  # "standard" ë˜ëŠ” "hd"
    style="vivid",  # "vivid" ë˜ëŠ” "natural"
    n=1  # DALL-E 3ëŠ” n=1ë§Œ ì§€ì›
)

# ìƒì„±ëœ ì´ë¯¸ì§€ URL í™•ì¸
image_url = response.data[0].url
print(f"ìƒì„±ëœ ì´ë¯¸ì§€ URL: {image_url}")

# ìˆ˜ì •ëœ í”„ë¡¬í”„íŠ¸ í™•ì¸ (DALL-E 3ì˜ í”„ë¡¬í”„íŠ¸ ìµœì í™”)
revised_prompt = response.data[0].revised_prompt
print(f"ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸: {revised_prompt}")

# ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë° ì €ì¥
import requests
from PIL import Image
import io

# URLì€ ì„ì‹œì ì´ë¯€ë¡œ ì¦‰ì‹œ ë‹¤ìš´ë¡œë“œ ê¶Œì¥
img_response = requests.get(image_url)
image = Image.open(io.BytesIO(img_response.content))
image.save('images/gangnam_image.png', 'PNG')
```

![ê°•ë‚¨ê±°ë¦¬ ì§€ë¸Œë¦¬ ìŠ¤íƒ€ì¼ë¡œ ìƒì„±ëœ ì´ë¯¸ì§€](images/gangnam_image.png){#fig-dalle-gangnam fig-align="center" width="200"}

### DALLÂ·E 3 ì˜µì…˜

DALLÂ·E 3ì˜ ë‹¤ì–‘í•œ ì˜µì…˜ë“¤ì„ ì ì ˆíˆ ì¡°í•©í•˜ë©´ ìš©ë„ì— ë§ëŠ” ìµœì ì˜ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•  ìˆ˜ ìˆë‹¤. í¬ê¸°ëŠ” ì‚¬ìš© ëª©ì ì— ë”°ë¼, í’ˆì§ˆì€ ì˜ˆì‚°ê³¼ ìš”êµ¬ì‚¬í•­ì— ë”°ë¼, ìŠ¤íƒ€ì¼ì€ ë¸Œëœë“œë‚˜ ì½˜í…ì¸  ì„±ê²©ì— ë§ì¶° ì„ íƒí•œë‹¤.

**ì´ë¯¸ì§€ í¬ê¸° ì„¤ì •**ì€ ìµœì¢… ìš©ë„ë¥¼ ê³ ë ¤í•˜ì—¬ ê²°ì •í•´ì•¼ í•œë‹¤. DALLÂ·E 3ëŠ” ì„¸ ê°€ì§€ í•´ìƒë„ë¥¼ ì§€ì›í•˜ë©°, ì •ì‚¬ê°í˜•(1024x1024)ì€ ì†Œì…œ ë¯¸ë””ì–´ í”„ë¡œí•„ì´ë‚˜ ì•„ì´ì½˜ì— ì í•©í•˜ê³  ì²˜ë¦¬ ì†ë„ê°€ ê°€ì¥ ë¹ ë¥´ë‹¤. ê°€ë¡œí˜•(1792x1024)ì€ ì›¹ì‚¬ì´íŠ¸ ë°°ë„ˆë‚˜ í”„ë ˆì  í…Œì´ì…˜ ìŠ¬ë¼ì´ë“œì— ì´ìƒì ì´ë©°, ì„¸ë¡œí˜•(1024x1792)ì€ ëª¨ë°”ì¼ í™”ë©´ì´ë‚˜ í¬ìŠ¤í„° ì œì‘ì— ìœ ìš©í•˜ë‹¤. DALLÂ·E 2ì˜ ê²½ìš° 256x256ë¶€í„° 1024x1024ê¹Œì§€ ë‹¤ì–‘í•œ í¬ê¸°ë¥¼ ì œê³µí•˜ì§€ë§Œ, í’ˆì§ˆ ë©´ì—ì„œëŠ” DALLÂ·E 3ì— ë¹„í•´ ì œí•œì ì´ë‹¤.

**í’ˆì§ˆ ì˜µì…˜**ì€ ë¹„ìš©ê³¼ ê²°ê³¼ë¬¼ì˜ ì™„ì„±ë„ ì‚¬ì´ì˜ ê· í˜•ì„ ë§ì¶”ëŠ” ì¤‘ìš”í•œ ìš”ì†Œë‹¤. í‘œì¤€ í’ˆì§ˆ(standard)ì€ ì¼ë°˜ì ì¸ ìš©ë„ì— ì¶©ë¶„í•˜ë©° ë¹ ë¥¸ ì²˜ë¦¬ì™€ ì €ë ´í•œ ë¹„ìš©ì´ ì¥ì ì´ë‹¤. ê³ í’ˆì§ˆ(hd) ì˜µì…˜ì€ ì„¸ë¶€ì‚¬í•­ì˜ ì •ë°€ë„ë¥¼ ë†’ì´ê³  ì´ë¯¸ì§€ ì „ì²´ì˜ ì¼ê´€ì„±ì„ ê°œì„ í•˜ì—¬ ì „ë¬¸ì ì¸ ìš©ë„ë‚˜ ì¸ì‡„ë¬¼ ì œì‘ ì‹œ ê¶Œì¥ëœë‹¤. ë¹„ìš©ì€ ì•½ 2ë°° ì°¨ì´ê°€ ë‚˜ë¯€ë¡œ í”„ë¡œì íŠ¸ì˜ ìš”êµ¬ì‚¬í•­ê³¼ ì˜ˆì‚°ì„ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤í•´ì•¼ í•œë‹¤.

**ìŠ¤íƒ€ì¼ ì„¤ì •**ì€ ì´ë¯¸ì§€ì˜ ì „ë°˜ì ì¸ ë¶„ìœ„ê¸°ì™€ í‘œí˜„ ë°©ì‹ì„ ê²°ì •í•œë‹¤. ìƒìƒí•œ(vivid) ìŠ¤íƒ€ì¼ì€ ì±„ë„ê°€ ë†’ê³  ëŒ€ë¹„ê°€ ê°•í•œ ì˜í™”ì  í‘œí˜„ì„ ì œê³µí•˜ì—¬ ì‹œê°ì  ì„íŒ©íŠ¸ê°€ í° ë§ˆì¼€íŒ… ìë£Œë‚˜ ì°½ì‘ë¬¼ì— ì í•©í•˜ë‹¤. ìì—°ìŠ¤ëŸ¬ìš´(natural) ìŠ¤íƒ€ì¼ì€ ì‚¬ì‹¤ì ì´ê³  ì ˆì œëœ í‘œí˜„ìœ¼ë¡œ êµìœ¡ ìë£Œë‚˜ ê¸°ìˆ  ë¬¸ì„œì—ì„œ ë” ì ì ˆí•˜ë‹¤. ë¸Œëœë“œ ê°€ì´ë“œë¼ì¸ì´ë‚˜ ì½˜í…ì¸ ì˜ ì„±ê²©ì— ë”°ë¼ ì¼ê´€ëœ ìŠ¤íƒ€ì¼ì„ ìœ ì§€í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤.

::: callout-note
### ì‚¬ìš© ì œí•œì‚¬í•­

ì œí•œì‚¬í•­ë“¤ì„ ë¯¸ë¦¬ íŒŒì•…í•˜ê³  ê³„íší•˜ë©´ í”„ë¡œì íŠ¸ ì§„í–‰ ì‹œ ì˜ˆìƒì¹˜ ëª»í•œ ë¬¸ì œë¥¼ ë°©ì§€í•  ìˆ˜ ìˆë‹¤. íŠ¹íˆ ëŒ€ëŸ‰ì˜ ì´ë¯¸ì§€ê°€ í•„ìš”í•œ í”„ë¡œì íŠ¸ì—ì„œëŠ” ì†ë„ ì œí•œì„ ê³ ë ¤í•œ ì¼ì • ê³„íšì´ í•„ìš”í•˜ë‹¤.

-   **ì†ë„ ì œí•œ**: í‘œì¤€ ê³„ì •ì€ ë¶„ë‹¹ 5ê°œ ìš”ì²­
-   **ì´ë¯¸ì§€ ìˆ˜**: DALL-E 3ëŠ” í•œ ë²ˆì— 1ê°œ ì´ë¯¸ì§€ë§Œ ìƒì„± (n=1)
-   **ì„ì‹œ ì €ì¥**: ìƒì„±ëœ ì´ë¯¸ì§€ URLì€ ì„ì‹œì ì´ë¯€ë¡œ ì¦‰ì‹œ ë‹¤ìš´ë¡œë“œ í•„ìš”
-   **í¸ì§‘ ë¶ˆê°€**: DALL-E 3ëŠ” ì´ë¯¸ì§€ í¸ì§‘ì´ë‚˜ ë³€í˜• API ë¯¸ì§€ì›

:::

Base64 ì¸ì½”ë”© ë°©ì‹ì€ ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ì§ì ‘ ë°›ì•„ì˜¬ ìˆ˜ ìˆì–´ URL ë§Œë£Œ ê±±ì • ì—†ì´ ì•ˆì •ì ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆë‹¤. íŠ¹íˆ ì„œë²„ í™˜ê²½ì—ì„œ ì´ë¯¸ì§€ë¥¼ ì¦‰ì‹œ ì²˜ë¦¬í•˜ê³  ì €ì¥í•´ì•¼ í•  ë•Œ ìœ ìš©í•˜ë‹¤.

```{python}
#| eval: false
# Base64 ì¸ì½”ë”©ëœ ì´ë¯¸ì§€ë¡œ ë°›ê¸°
response_format = "b64_json" 

response = client.images.generate(
    model="dall-e-3",
    prompt="ì•„ë¦„ë‹¤ìš´ í•œêµ­ ì „í†µ í•œì˜¥",
    response_format=response_format
)

# Base64 ë°ì´í„°ë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜
import base64
from io import BytesIO

if response_format == "b64_json":
    image_data = base64.b64decode(response.data[0].b64_json)
    image = Image.open(BytesIO(image_data))
    image.save('images/hanok.png')

```

![ì‘ë‹µí˜•ì‹ì„ Base64 ë°ì´í„°ë¡œ ìƒì„±ëœ ì´ë¯¸ì§€](images/hanok.png){#fig-dalle-hanok fig-align="center" width="200"}



## ì„ë² ë”©

í…ìŠ¤íŠ¸ ì„ë² ë”©ì€ ìì—°ì–´ë¥¼ ìˆ˜ì¹˜ì  ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ ì»´í“¨í„°ê°€ ì´í•´í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë§Œë“œëŠ” í•µì‹¬ ê¸°ìˆ ì´ë‹¤. ë¬¸ì„œ ê²€ìƒ‰, ì¶”ì²œ ì‹œìŠ¤í…œ, ê°ì • ë¶„ì„, í´ëŸ¬ìŠ¤í„°ë§ ë“± ë‹¤ì–‘í•œ AI ì‘ìš©ì—ì„œ ê¸°ë°˜ ê¸°ìˆ ë¡œ í™œìš©ëœë‹¤. ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ í…ìŠ¤íŠ¸ë“¤ì€ ë²¡í„° ê³µê°„ì—ì„œë„ ê°€ê¹Œìš´ ìœ„ì¹˜ì— ë°°ì¹˜ë˜ì–´ ìœ ì‚¬ë„ ì¸¡ì •ì´ ê°€ëŠ¥í•˜ë‹¤. OpenAI Embeddings API [@openai2025embeddings]ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ê³ ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜í•  ìˆ˜ ìˆë‹¤.

::: callout-note
### ì‚¬ìš© ê°€ëŠ¥í•œ ì„ë² ë”© ëª¨ë¸

- `text-embedding-3-large`: ìµœê³  ì„±ëŠ¥ ëª¨ë¸ (3072ì°¨ì›, $0.00013/1K í† í°)
- `text-embedding-3-small`: íš¨ìœ¨ì ì´ê³  ë¹„ìš© íš¨ê³¼ì  (1536ì°¨ì›, ë¹ ë¥¸ ì²˜ë¦¬)
- `text-embedding-ada-002`: ì´ì „ ëª¨ë¸ (1536ì°¨ì›, í˜¸í™˜ì„± ëª©ì )

:::

### ê¸°ë³¸ ì„ë² ë”© ìƒì„±

ì„ë² ë”© ìƒì„±ì˜ í•µì‹¬ì€ ì ì ˆí•œ ëª¨ë¸ ì„ íƒê³¼ ì¼ê´€ëœ ì²˜ë¦¬ ë°©ì‹ì´ë‹¤. í”„ë¡œì íŠ¸ ì „ì²´ì—ì„œ ë™ì¼í•œ ëª¨ë¸ì„ ì‚¬ìš©í•´ì•¼ ë²¡í„° ê°„ ë¹„êµê°€ ì˜ë¯¸ ìˆëŠ” ê²°ê³¼ë¥¼ ì œê³µí•œë‹¤.

```{python}
#| eval: false
# í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± í•¨ìˆ˜
def get_embedding(text, model="text-embedding-3-large"):
    response = client.embeddings.create(
        input=text,
        model=model
    )
    return response.data[0].embedding

# ì˜ˆì œ: ë„ì‹œ ì´ë¦„ ì„ë² ë”©
seoul_embedding = get_embedding("ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì„œìš¸ì…ë‹ˆë‹¤.")
tokyo_embedding = get_embedding("ì¼ë³¸ì˜ ìˆ˜ë„ëŠ” ë„ì¿„ì…ë‹ˆë‹¤.")
paris_embedding = get_embedding("í”„ë‘ìŠ¤ì˜ ìˆ˜ë„ëŠ” íŒŒë¦¬ì…ë‹ˆë‹¤.")

print(f"ë²¡í„° ì°¨ì›: {len(seoul_embedding)}")  # 3072ì°¨ì› (text-embedding-3-large)

# ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
import numpy as np

def cosine_similarity(a, b):
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

# ì„œìš¸-ë„ì¿„ ìœ ì‚¬ë„
similarity_seoul_tokyo = cosine_similarity(seoul_embedding, tokyo_embedding)
print(f"ì„œìš¸-ë„ì¿„ ìœ ì‚¬ë„: {similarity_seoul_tokyo:.4f}")

# ì„œìš¸-íŒŒë¦¬ ìœ ì‚¬ë„
similarity_seoul_paris = cosine_similarity(seoul_embedding, paris_embedding)
print(f"ì„œìš¸-íŒŒë¦¬ ìœ ì‚¬ë„: {similarity_seoul_paris:.4f}")
```

```markdown
ë²¡í„° ì°¨ì›: 3072
ì„œìš¸-ë„ì¿„ ìœ ì‚¬ë„: 0.5272
ì„œìš¸-íŒŒë¦¬ ìœ ì‚¬ë„: 0.5015
```

### ì°¨ì› ì¶•ì†Œ ê¸°ëŠ¥

Matryoshka Representation Learning ê¸°ìˆ ì„ í™œìš©í•œ ì°¨ì› ì¶•ì†ŒëŠ” ì €ì¥ ê³µê°„ê³¼ ê³„ì‚° ë¹„ìš©ì„ í¬ê²Œ ì ˆì•½í•˜ë©´ì„œë„ ì„±ëŠ¥ ì†ì‹¤ì„ ìµœì†Œí™”í•˜ëŠ” í˜ì‹ ì ì¸ ê¸°ëŠ¥ì´ë‹¤. ëŒ€ê·œëª¨ ë¬¸ì„œ ì»¬ë ‰ì…˜ì„ ë‹¤ë£° ë•Œ íŠ¹íˆ ìœ ìš©í•˜ë©°, ë©”ëª¨ë¦¬ ì œì•½ì´ ìˆëŠ” í™˜ê²½ì—ì„œë„ íš¨ê³¼ì ìœ¼ë¡œ ì„ë² ë”©ì„ í™œìš©í•  ìˆ˜ ìˆë‹¤. ìƒˆë¡œìš´ ì„ë² ë”© ëª¨ë¸ì€ ì°¨ì›ì„ ì¤„ì—¬ë„ ì„±ëŠ¥ì´ í¬ê²Œ ë–¨ì–´ì§€ì§€ ì•ŠëŠ”ë‹¤:

```{python}
#| eval: false
# ì°¨ì› ì¶•ì†Œ ì„ë² ë”© ìƒì„±
def get_embedding_with_dimensions(text, dimensions=1024):
    response = client.embeddings.create(
        input=text,
        model="text-embedding-3-large",
        dimensions=dimensions  # 256, 512, 1024, 2048 ë“± ì„ íƒ ê°€ëŠ¥
    )
    return response.data[0].embedding

# ì°¨ì›ë³„ ì„±ëŠ¥ ë¹„êµ
text = "ì¸ê³µì§€ëŠ¥ê³¼ ë¨¸ì‹ ëŸ¬ë‹ì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?"

# í’€ ì‚¬ì´ì¦ˆ (3072ì°¨ì›)
full_embedding = get_embedding(text, "text-embedding-3-large")
print(f"í’€ ì‚¬ì´ì¦ˆ: {len(full_embedding)}ì°¨ì›")

# ì¶•ì†Œëœ ì‚¬ì´ì¦ˆ (1024ì°¨ì›)
reduced_embedding = get_embedding_with_dimensions(text, 1024)
print(f"ì¶•ì†Œ ì‚¬ì´ì¦ˆ: {len(reduced_embedding)}ì°¨ì›")

# ì €ì¥ ê³µê°„ 14ë°° ì ˆì•½, ì„±ëŠ¥ì€ ê±°ì˜ ë™ì¼
```

```markdown
í’€ ì‚¬ì´ì¦ˆ: 3072ì°¨ì›
ì¶•ì†Œ ì‚¬ì´ì¦ˆ: 1024ì°¨ì›
```

### ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ

ê° ëª¨ë¸ì€ ì„±ëŠ¥ê³¼ ë¹„ìš© ì¸¡ë©´ì—ì„œ ê³ ìœ í•œ íŠ¹ì„±ì„ ê°€ì§€ê³  ìˆì–´, í”„ë¡œì íŠ¸ì˜ ìš”êµ¬ì‚¬í•­ì— ë§ëŠ” ì„ íƒì´ ì¤‘ìš”í•˜ë‹¤. ë†’ì€ ì •í™•ë„ê°€ í•„ìš”í•œ í•µì‹¬ ê¸°ëŠ¥ì—ëŠ” large ëª¨ë¸ì„, ëŒ€ëŸ‰ ì²˜ë¦¬ê°€ í•„ìš”í•œ ë°°ì¹˜ ì‘ì—…ì—ëŠ” small ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ íš¨ìœ¨ì ì´ë‹¤.

```{python}
#| eval: false
# ëª¨ë¸ë³„ ì„±ëŠ¥ ë° ë¹„ìš© ë¹„êµ
models = [
    {"name": "text-embedding-3-large", "dimensions": 3072, "price": 0.00013},
    {"name": "text-embedding-3-small", "dimensions": 1536, "price": 0.00002},
    {"name": "text-embedding-ada-002", "dimensions": 1536, "price": 0.0001}
]

test_text = "ìì—°ì–´ ì²˜ë¦¬ì™€ ì»´í“¨í„° ë¹„ì „ì˜ ì‘ìš© ë¶„ì•¼"

for model in models:
    embedding = get_embedding(test_text, model["name"])
    print(f"ëª¨ë¸: {model['name']}")
    print(f"ì°¨ì›ìˆ˜: {len(embedding)}")
    print(f"ê°€ê²©: ${model['price']}/1K í† í°")
    print("---")
```

```markdown
ëª¨ë¸: text-embedding-3-large
ì°¨ì›ìˆ˜: 3072
ê°€ê²©: $0.00013/1K í† í°
---
ëª¨ë¸: text-embedding-3-small
ì°¨ì›ìˆ˜: 1536
ê°€ê²©: $2e-05/1K í† í°
---
ëª¨ë¸: text-embedding-ada-002
ì°¨ì›ìˆ˜: 1536
ê°€ê²©: $0.0001/1K í† í°
---
```

### ì‹¤ìš©ì ì¸ ìœ ì‚¬ë„ ê²€ìƒ‰ ì‹œìŠ¤í…œ

ì‹¤ì œ ì‘ìš©ì—ì„œëŠ” ë‹¨ìˆœí•œ ì„ë² ë”© ìƒì„±ì„ ë„˜ì–´ì„œ ì²´ê³„ì ì¸ ê²€ìƒ‰ ì‹œìŠ¤í…œ êµ¬ì¶•ì´ í•„ìš”í•˜ë‹¤. ë¬¸ì„œë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜í•˜ê³  ì¸ë±ì‹±í•˜ì—¬ ë¹ ë¥¸ ê²€ìƒ‰ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ë©°, ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ í†µí•´ ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œë¥¼ ìˆœì„œëŒ€ë¡œ ì œê³µí•œë‹¤.

```{python}
#| eval: false
# ë¬¸ì„œ ìœ ì‚¬ë„ ê²€ìƒ‰ ì‹œìŠ¤í…œ
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity

def build_document_search_system(documents):
    """ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ê²€ìƒ‰ ì‹œìŠ¤í…œ êµ¬ì¶•"""
    embeddings = []
    for doc in documents:
        embedding = get_embedding(doc, "text-embedding-3-small")
        embeddings.append(embedding)
    
    return pd.DataFrame({
        'document': documents,
        'embedding': embeddings
    })

def search_similar_documents(query, doc_df, top_k=3):
    """ì¿¼ë¦¬ì™€ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œë“¤ ê²€ìƒ‰"""
    query_embedding = get_embedding(query, "text-embedding-3-small")
    
    # ëª¨ë“  ë¬¸ì„œì™€ì˜ ìœ ì‚¬ë„ ê³„ì‚°
    similarities = []
    for embedding in doc_df['embedding']:
        similarity = cosine_similarity([query_embedding], [embedding])[0][0]
        similarities.append(similarity)
    
    doc_df['similarity'] = similarities
    return doc_df.nlargest(top_k, 'similarity')

# ì‚¬ìš© ì˜ˆì œ
documents = [
    "íŒŒì´ì¬ì€ ë°ì´í„° ë¶„ì„ì— ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤.",
    "ë¨¸ì‹ ëŸ¬ë‹ì€ ì¸ê³µì§€ëŠ¥ì˜ í•œ ë¶„ì•¼ë¡œ ë°ì´í„°ë¡œë¶€í„° íŒ¨í„´ì„ í•™ìŠµí•©ë‹ˆë‹¤.",
    "ìì—°ì–´ ì²˜ë¦¬ëŠ” ì»´í“¨í„°ê°€ ì¸ê°„ì˜ ì–¸ì–´ë¥¼ ì´í•´í•˜ë„ë¡ í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.",
    "ë”¥ëŸ¬ë‹ì€ ì‹ ê²½ë§ì„ ì´ìš©í•œ ë¨¸ì‹ ëŸ¬ë‹ì˜ í•œ ë°©ë²•ì…ë‹ˆë‹¤."
]

doc_system = build_document_search_system(documents)
results = search_similar_documents("AIì™€ ë°ì´í„° ê³¼í•™", doc_system)
print(results[['document', 'similarity']])
```

```markdown
                               document  similarity
0     íŒŒì´ì¬ì€ ë°ì´í„° ë¶„ì„ì— ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤.    0.417869
1   ë¨¸ì‹ ëŸ¬ë‹ì€ ì¸ê³µì§€ëŠ¥ì˜ í•œ ë¶„ì•¼ë¡œ ë°ì´í„°ë¡œë¶€í„° íŒ¨í„´ì„ í•™ìŠµí•©ë‹ˆë‹¤.    0.395203
2  ìì—°ì–´ ì²˜ë¦¬ëŠ” ì»´í“¨í„°ê°€ ì¸ê°„ì˜ ì–¸ì–´ë¥¼ ì´í•´í•˜ë„ë¡ í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.    0.263060
```

## ì½˜í…ì¸  ì¤‘ì¬

ì˜¨ë¼ì¸ í”Œë«í¼ê³¼ ì»¤ë®¤ë‹ˆí‹°ì—ì„œ ì•ˆì „í•œ í™˜ê²½ì„ ìœ ì§€í•˜ëŠ” ê²ƒì€ í•„ìˆ˜ì ì¸ ìš”êµ¬ì‚¬í•­ì´ë‹¤. ìˆ˜ë™ìœ¼ë¡œ ëª¨ë“  ì½˜í…ì¸ ë¥¼ ê²€í† í•˜ëŠ” ê²ƒì€ í˜„ì‹¤ì ìœ¼ë¡œ ë¶ˆê°€ëŠ¥í•˜ë¯€ë¡œ, AI ê¸°ë°˜ ìë™ ì¤‘ì¬ ì‹œìŠ¤í…œì´ ì¤‘ìš”í•œ ì—­í• ì„ í•œë‹¤. OpenAIì˜ ì¤‘ì¬ APIëŠ” í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ë¥¼ ëª¨ë‘ ì²˜ë¦¬í•  ìˆ˜ ìˆì–´ ì¢…í•©ì ì¸ ì½˜í…ì¸  ì•ˆì „ ê´€ë¦¬ê°€ ê°€ëŠ¥í•˜ë‹¤. OpenAI Moderation API [@openai2025moderation]ëŠ” ì½˜í…ì¸ ì˜ ì•ˆì „ì„±ì„ ê²€ì‚¬í•˜ì—¬ ìœ í•´í•œ ë‚´ìš©ì„ íƒì§€í•œë‹¤.

### ì¤‘ì¬ ëª¨ë¸ê³¼ íƒì§€ ì‹œìŠ¤í…œ

OpenAIì˜ ì½˜í…ì¸  ì¤‘ì¬ ì‹œìŠ¤í…œì€ 2025ë…„ GPT-4o ê¸°ë°˜ìœ¼ë¡œ ì™„ì „íˆ ìƒˆë¡­ê²Œ ê°œí¸ë˜ì–´ ì´ì „ ë²„ì „ë³´ë‹¤ í¬ê²Œ í–¥ìƒëœ ì„±ëŠ¥ì„ ì œê³µí•œë‹¤. ì´ ì‹œìŠ¤í…œì€ ë‹¨ìˆœí•œ í‚¤ì›Œë“œ í•„í„°ë§ì„ ë„˜ì–´ì„œ ë§¥ë½ì„ ì´í•´í•˜ëŠ” AI ê¸°ë°˜ ì ‘ê·¼ë²•ì„ ì‚¬ìš©í•˜ì—¬ ë”ìš± ì •í™•í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì½˜í…ì¸  ë¶„ë¥˜ë¥¼ ìˆ˜í–‰í•œë‹¤. ê°€ì¥ ì£¼ëª©í•  ë§Œí•œ ê°œì„ ì‚¬í•­ì€ ë©€í‹°ëª¨ë‹¬ ì§€ì›ìœ¼ë¡œ, í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ë¥¼ ë™ì‹œì— ì²˜ë¦¬í•  ìˆ˜ ìˆì–´ ì¢…í•©ì ì¸ ì½˜í…ì¸  ì•ˆì „ ê´€ë¦¬ê°€ ê°€ëŠ¥í•´ì¡Œë‹¤.

ì‹œìŠ¤í…œì€ 8ê°€ì§€ ì£¼ìš” ì¹´í…Œê³ ë¦¬ë¡œ ìœ í•´ ì½˜í…ì¸ ë¥¼ ë¶„ë¥˜í•œë‹¤. ê¸°ì¡´ì˜ ì¦ì˜¤ í‘œí˜„(`hate`), ê´´ë¡­í˜(`harassment`), ìí•´(`self-harm`), ì„±ì  ì½˜í…ì¸ (`sexual`), í­ë ¥(`violence`, `violence/graphic`) ì¹´í…Œê³ ë¦¬ì— ë”í•´, 2025ë…„ì—ëŠ” ë¶ˆë²• í–‰ìœ„ ê´€ë ¨ ì¹´í…Œê³ ë¦¬(`illicit`, `illicit/violent`)ê°€ ìƒˆë¡­ê²Œ ì¶”ê°€ë˜ì—ˆë‹¤. ì´ëŸ¬í•œ ì¹´í…Œê³ ë¦¬ í™•ì¥ì€ ë²•ì  ë¦¬ìŠ¤í¬ ë°©ì§€ì™€ ë” í¬ê´„ì ì¸ ì•ˆì „ ê´€ë¦¬ë¥¼ ìœ„í•œ í•„ìˆ˜ì ì¸ ë°œì „ì´ë‹¤. ê° ì¹´í…Œê³ ë¦¬ëŠ” 0ê³¼ 1 ì‚¬ì´ì˜ ì ìˆ˜ë¡œ ìœ„í—˜ë„ë¥¼ ì œê³µí•˜ì—¬ ê°œë°œìê°€ ìì‹ ì˜ í”Œë«í¼ì— ë§ëŠ” ì„ê³„ê°’ì„ ì„¤ì •í•  ìˆ˜ ìˆë„ë¡ ì§€ì›í•œë‹¤.

ë‹¤êµ­ì–´ ì§€ì› ì—­ì‹œ í¬ê²Œ ê°œì„ ë˜ì–´ 40ê°œ ì–¸ì–´ì—ì„œ í‰ê·  42%ì˜ ì„±ëŠ¥ í–¥ìƒì„ ë‹¬ì„±í–ˆë‹¤. ì´ëŠ” ê¸€ë¡œë²Œ ì„œë¹„ìŠ¤ ìš´ì˜ì—ì„œ ì–¸ì–´ë³„ ë¬¸í™”ì  ë§¥ë½ê³¼ í‘œí˜„ ë°©ì‹ì˜ ì°¨ì´ë¥¼ ë” ì •í™•íˆ ì´í•´í•  ìˆ˜ ìˆìŒì„ ì˜ë¯¸í•œë‹¤. íŠ¹íˆ í•œêµ­ì–´, ì¼ë³¸ì–´, ì¤‘êµ­ì–´ì™€ ê°™ì€ ì•„ì‹œì•„ ì–¸ì–´ì—ì„œì˜ ì„±ëŠ¥ ê°œì„ ì€ êµ­ë‚´ ì„œë¹„ìŠ¤ ìš´ì˜ìë“¤ì—ê²Œ í° ë„ì›€ì´ ë  ê²ƒì´ë‹¤.

### í…ìŠ¤íŠ¸ ì¤‘ì¬ ì‹¤ìŠµê³¼ í™œìš©

í…ìŠ¤íŠ¸ ì¤‘ì¬ëŠ” ì‹¤ì‹œê°„ ëŒ“ê¸€ ì‹œìŠ¤í…œ, ì†Œì…œ ë¯¸ë””ì–´ í”Œë«í¼, ì±„íŒ… ì• í”Œë¦¬ì¼€ì´ì…˜ ë“±ì—ì„œ í•µì‹¬ì ì¸ ì—­í• ì„ ìˆ˜í–‰í•œë‹¤. OpenAIì˜ ì¤‘ì¬ APIëŠ” ë‹¨ìˆœí•œ ì´ì§„ ë¶„ë¥˜(ì•ˆì „/ìœ„í—˜)ë¥¼ ë„˜ì–´ì„œ ê° ì¹´í…Œê³ ë¦¬ë³„ ì„¸ë°€í•œ ì ìˆ˜ë¥¼ ì œê³µí•˜ë¯€ë¡œ, í”Œë«í¼ì˜ íŠ¹ì„±ê³¼ ì •ì±…ì— ë§ëŠ” ë§ì¶¤í˜• ì¤‘ì¬ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìˆë‹¤.

ì•ˆì „í•œ ì½˜í…ì¸ ì˜ ê²½ìš° ì¼ë°˜ì ì¸ ì¼ìƒ ëŒ€í™”, êµìœ¡ì  ë‚´ìš©, ì—…ë¬´ ê´€ë ¨ ì†Œí†µ ë“±ì´ ëª¨ë“  ìœ„í—˜ ì¹´í…Œê³ ë¦¬ì—ì„œ ë‚®ì€ ì ìˆ˜ë¥¼ ë°›ëŠ”ë‹¤. ë°˜ë©´ ëª…ì‹œì ìœ¼ë¡œ ìœ í•´í•œ ì½˜í…ì¸ ëŠ” í•´ë‹¹í•˜ëŠ” íŠ¹ì • ì¹´í…Œê³ ë¦¬ì—ì„œ ë†’ì€ ì ìˆ˜ë¥¼ ë°›ì•„ ìë™ìœ¼ë¡œ í”Œë˜ê·¸ëœë‹¤. ê°€ì¥ í¥ë¯¸ë¡œìš´ ë¶€ë¶„ì€ ê²½ê³„ì„  ì‚¬ë¡€ë“¤ë¡œ, ë§¥ë½ì— ë”°ë¼ ë‹¤ë¥´ê²Œ í•´ì„ë  ìˆ˜ ìˆëŠ” ë‚´ìš©ë“¤ì´ë‹¤.

```{python}
#| eval: false
# ì•ˆì „í•œ í…ìŠ¤íŠ¸ ì¤‘ì¬
safe_text = "ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì •ë§ ì¢‹ë„¤ìš”. ë°ì´í„° ë¶„ì„ì— ëŒ€í•´ ë°°ìš°ê³  ì‹¶ìŠµë‹ˆë‹¤."

response = client.moderations.create(
    input=safe_text,
    model="omni-moderation-latest"
)

result = response.results[0]
print(f"ì…ë ¥ í…ìŠ¤íŠ¸: {safe_text}")
print(f"ìœ„í—˜ í”Œë˜ê·¸: {result.flagged}")

# Categories ê°ì²´ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
categories_dict = dict(result.categories)
print(f"ëª¨ë“  ì¹´í…Œê³ ë¦¬ ì•ˆì „: {not any(categories_dict.values())}")
```

```markdown
ì…ë ¥ í…ìŠ¤íŠ¸: ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì •ë§ ì¢‹ë„¤ìš”. ë°ì´í„° ë¶„ì„ì— ëŒ€í•´ ë°°ìš°ê³  ì‹¶ìŠµë‹ˆë‹¤.
ìœ„í—˜ í”Œë˜ê·¸: False
ëª¨ë“  ì¹´í…Œê³ ë¦¬ ì•ˆì „: True
```

ì‹¤ì œ ìš´ì˜ í™˜ê²½ì—ì„œëŠ” ë‹¤ì–‘í•œ ìœ í˜•ì˜ ìœ í•´ ì½˜í…ì¸ ë¥¼ ë§Œë‚  ìˆ˜ ìˆìœ¼ë©°, ê°ê°ì€ ì„œë¡œ ë‹¤ë¥¸ ìœ„í—˜ ì¹´í…Œê³ ë¦¬ì—ì„œ ë†’ì€ ì ìˆ˜ë¥¼ ë°›ëŠ”ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ í˜ì˜¤ í‘œí˜„ì€ `hate`ì™€ `harassment` ì¹´í…Œê³ ë¦¬ì—ì„œ, í­ë ¥ì  ë‚´ìš©ì€ `violence` ì¹´í…Œê³ ë¦¬ì—ì„œ, ìí•´ ê´€ë ¨ ë‚´ìš©ì€ `self-harm` ì¹´í…Œê³ ë¦¬ì—ì„œ ë†’ì€ ì ìˆ˜ë¥¼ ë°›ëŠ”ë‹¤. ìƒˆë¡­ê²Œ ì¶”ê°€ëœ `illicit` ì¹´í…Œê³ ë¦¬ëŠ” ë¶ˆë²• í–‰ìœ„ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ì§€ì‹œì‚¬í•­ì„ íƒì§€í•˜ì—¬ ë²•ì  ë¬¸ì œë¥¼ ì‚¬ì „ì— ë°©ì§€í•œë‹¤.

```{python}
#| eval: false
# ìœ„í—˜í•œ í…ìŠ¤íŠ¸ë“¤ (êµìœ¡ ëª©ì )
test_texts = [
    "íŠ¹ì • ì§‘ë‹¨ì„ í–¥í•œ í˜ì˜¤ í‘œí˜„ì´ ë‹´ê¸´ í…ìŠ¤íŠ¸",
    "í­ë ¥ì ì¸ ë‚´ìš©ì´ í¬í•¨ëœ ìœ„í˜‘ì  ë©”ì‹œì§€",
    "ìí•´ë¥¼ ì¡°ì¥í•˜ëŠ” ë‚´ìš©",
    "ë¶ˆë²•ì ì¸ í–‰ìœ„ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ì§€ì‹œì‚¬í•­"
]

for i, text in enumerate(test_texts, 1):
    print(f"\n=== í…ŒìŠ¤íŠ¸ {i} ===")
    
    response = client.moderations.create(
        input=text,
        model="omni-moderation-latest"
    )
    
    result = response.results[0]
    print(f"í…ìŠ¤íŠ¸: {text}")
    print(f"ìœ„í—˜ í”Œë˜ê·¸: {result.flagged}")
    
    # ìœ„ë°˜ëœ ì¹´í…Œê³ ë¦¬ë§Œ í‘œì‹œ (Categories ê°ì²´ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜)
    categories_dict = dict(result.categories)
    violated_categories = [cat for cat, violated in categories_dict.items() if violated]
    if violated_categories:
        print(f"ìœ„ë°˜ ì¹´í…Œê³ ë¦¬: {', '.join(violated_categories)}")
    
    # ë†’ì€ ì ìˆ˜ì˜ ì¹´í…Œê³ ë¦¬ í‘œì‹œ (CategoryScores ê°ì²´ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜)
    scores_dict = dict(result.category_scores)
    high_scores = {cat: score for cat, score in scores_dict.items() if score > 0.1}
    if high_scores:
        print("ë†’ì€ ìœ„í—˜ë„ ì ìˆ˜:")
        for cat, score in sorted(high_scores.items(), key=lambda x: x[1], reverse=True):
            print(f"  {cat}: {score:.4f}")
```

```markdown
=== í…ŒìŠ¤íŠ¸ 1 ===
í…ìŠ¤íŠ¸: íŠ¹ì • ì§‘ë‹¨ì„ í–¥í•œ í˜ì˜¤ í‘œí˜„ì´ ë‹´ê¸´ í…ìŠ¤íŠ¸
ìœ„í—˜ í”Œë˜ê·¸: False
ë†’ì€ ìœ„í—˜ë„ ì ìˆ˜:
  harassment: 0.1435

=== í…ŒìŠ¤íŠ¸ 2 ===
í…ìŠ¤íŠ¸: í­ë ¥ì ì¸ ë‚´ìš©ì´ í¬í•¨ëœ ìœ„í˜‘ì  ë©”ì‹œì§€
ìœ„í—˜ í”Œë˜ê·¸: True
ìœ„ë°˜ ì¹´í…Œê³ ë¦¬: violence
ë†’ì€ ìœ„í—˜ë„ ì ìˆ˜:
  violence: 0.3521

=== í…ŒìŠ¤íŠ¸ 3 ===
í…ìŠ¤íŠ¸: ìí•´ë¥¼ ì¡°ì¥í•˜ëŠ” ë‚´ìš©
ìœ„í—˜ í”Œë˜ê·¸: True
ìœ„ë°˜ ì¹´í…Œê³ ë¦¬: self_harm, self_harm_intent, self-harm/intent, self-harm
ë†’ì€ ìœ„í—˜ë„ ì ìˆ˜:
  self_harm: 0.4342
  self_harm_intent: 0.2915
  self_harm_instructions: 0.1795

=== í…ŒìŠ¤íŠ¸ 4 ===
í…ìŠ¤íŠ¸: ë¶ˆë²•ì ì¸ í–‰ìœ„ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ì§€ì‹œì‚¬í•­
ìœ„í—˜ í”Œë˜ê·¸: True
ìœ„ë°˜ ì¹´í…Œê³ ë¦¬: illicit
ë†’ì€ ìœ„í—˜ë„ ì ìˆ˜:
  illicit: 0.4992
```

íŠ¹íˆ ì£¼ëª©í•  ì ì€ ê²½ê³„ì„  ì‚¬ë¡€ë“¤ì´ë‹¤. ëª¨ë“  ì½˜í…ì¸ ê°€ ëª…í™•íˆ ì•ˆì „í•˜ê±°ë‚˜ ìœ„í—˜í•œ ê²ƒì€ ì•„ë‹ˆë©°, ë§¥ë½ì— ë”°ë¼ ë‹¤ë¥´ê²Œ í•´ì„ë  ìˆ˜ ìˆëŠ” ë‚´ìš©ë“¤ì€ ì¶”ê°€ì ì¸ ì¸ê°„ ê²€í† ë‚˜ ë” ì •êµí•œ ì •ì±… ì ìš©ì´ í•„ìš”í•˜ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ "ê²Œì„ì—ì„œ ì ì„ ë¬¼ë¦¬ì³¤ë‹¤"ëŠ” í‘œí˜„ì€ ê²Œì„ ë§¥ë½ì—ì„œëŠ” ìì—°ìŠ¤ëŸ½ì§€ë§Œ ì¤‘ì¬ ì‹œìŠ¤í…œì—ì„œëŠ” í­ë ¥ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜ë  ìˆ˜ ìˆë‹¤. ì´ëŸ¬í•œ ê²½ìš° ê°œë°œìëŠ” ì„ê³„ê°’ ì¡°ì •ì´ë‚˜ ì¶”ê°€ì ì¸ ë§¥ë½ ë¶„ì„ì„ í†µí•´ ì •í™•ì„±ì„ ë†’ì¼ ìˆ˜ ìˆë‹¤.

```{python}
#| eval: false
# ê²½ê³„ì„  ì‚¬ë¡€ë“¤ - ì»¨í…ìŠ¤íŠ¸ì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆëŠ” ë‚´ìš©
borderline_texts = [
    "ê²Œì„ì—ì„œ ì ì„ ë¬¼ë¦¬ì³¤ë‹¤!",  # ê²Œì„ í­ë ¥ vs ì‹¤ì œ í­ë ¥
    "ë‹¤ì´ì–´íŠ¸ë¥¼ ìœ„í•´ ì‹ì‚¬ë¥¼ ê±°ë¥´ê³  ìˆì–´ìš”",  # ê±´ê°•í•œ ë‹¤ì´ì–´íŠ¸ vs ìí•´
    "ì´ ì •ì±…ì— ê°•ë ¥íˆ ë°˜ëŒ€í•©ë‹ˆë‹¤",  # ì •ë‹¹í•œ ë¹„íŒ vs í˜ì˜¤
    "ìˆ ì„ ë§Œë“œëŠ” ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”"  # í•©ë²•ì  ì •ë³´ vs ë¶ˆë²• ì§€ì‹œ
]

print("ê²½ê³„ì„  ì‚¬ë¡€ ë¶„ì„:")
for text in borderline_texts:
    response = client.moderations.create(
        input=text,
        model="omni-moderation-latest"
    )
    
    result = response.results[0]
    print(f"\ní…ìŠ¤íŠ¸: '{text}'")
    print(f"í”Œë˜ê·¸: {result.flagged}")
    
    # 0.01 ì´ìƒì˜ ì ìˆ˜ë¥¼ ê°€ì§„ ì¹´í…Œê³ ë¦¬ í‘œì‹œ (CategoryScores ê°ì²´ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜)
    scores_dict = dict(result.category_scores)
    notable_scores = {cat: score for cat, score in scores_dict.items() if score > 0.01}
    if notable_scores:
        print("ì£¼ëª©í• ë§Œí•œ ì ìˆ˜:")
        for cat, score in sorted(notable_scores.items(), key=lambda x: x[1], reverse=True):
            print(f"  {cat}: {score:.4f}")
    else:
        print("ëª¨ë“  ì¹´í…Œê³ ë¦¬ì—ì„œ ë‚®ì€ ìœ„í—˜ë„")
```

```markdown
ê²½ê³„ì„  ì‚¬ë¡€ ë¶„ì„:

í…ìŠ¤íŠ¸: 'ê²Œì„ì—ì„œ ì ì„ ë¬¼ë¦¬ì³¤ë‹¤!'
í”Œë˜ê·¸: True
ì£¼ëª©í• ë§Œí•œ ì ìˆ˜:
  violence: 0.4290
  illicit: 0.0201

í…ìŠ¤íŠ¸: 'ë‹¤ì´ì–´íŠ¸ë¥¼ ìœ„í•´ ì‹ì‚¬ë¥¼ ê±°ë¥´ê³  ìˆì–´ìš”'
í”Œë˜ê·¸: False
ì£¼ëª©í• ë§Œí•œ ì ìˆ˜:
  self_harm: 0.0102

í…ìŠ¤íŠ¸: 'ì´ ì •ì±…ì— ê°•ë ¥íˆ ë°˜ëŒ€í•©ë‹ˆë‹¤'
í”Œë˜ê·¸: False
ëª¨ë“  ì¹´í…Œê³ ë¦¬ì—ì„œ ë‚®ì€ ìœ„í—˜ë„

í…ìŠ¤íŠ¸: 'ìˆ ì„ ë§Œë“œëŠ” ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”'
í”Œë˜ê·¸: True
ì£¼ëª©í• ë§Œí•œ ì ìˆ˜:
  illicit: 0.6243
  illicit_violent: 0.1666
```

### ë©€í‹°ëª¨ë‹¬ ì¤‘ì¬ì™€ ì´ë¯¸ì§€ ì²˜ë¦¬

ì´ë¯¸ì§€ ì¤‘ì¬ëŠ” 2025ë…„ OpenAI ì¤‘ì¬ ì‹œìŠ¤í…œì˜ ê°€ì¥ í˜ì‹ ì ì¸ ê¸°ëŠ¥ ì¤‘ í•˜ë‚˜ë‹¤. ê¸°ì¡´ í…ìŠ¤íŠ¸ ê¸°ë°˜ ì¤‘ì¬ì˜ í•œê³„ë¥¼ ë›°ì–´ë„˜ì–´ ì‹œê°ì  ì½˜í…ì¸ ê¹Œì§€ í¬ê´„í•˜ëŠ” ì¢…í•©ì ì¸ ì•ˆì „ ê´€ë¦¬ê°€ ê°€ëŠ¥í•´ì¡Œë‹¤. ì´ëŠ” ì†Œì…œ ë¯¸ë””ì–´, ì´ë¯¸ì§€ ê³µìœ  í”Œë«í¼, ì»¤ë®¤ë‹ˆí‹° ì‚¬ì´íŠ¸ ë“±ì—ì„œ í…ìŠ¤íŠ¸ë§Œìœ¼ë¡œëŠ” í¬ì°©í•˜ê¸° ì–´ë ¤ìš´ ë‹¤ì–‘í•œ í˜•íƒœì˜ ìœ í•´ ì½˜í…ì¸ ë¥¼ ìë™ìœ¼ë¡œ íƒì§€í•  ìˆ˜ ìˆìŒì„ ì˜ë¯¸í•œë‹¤.

ì´ë¯¸ì§€ ì¤‘ì¬ì—ì„œëŠ” í­ë ¥(`violence`, `violence/graphic`), ìí•´(`self-harm` ê´€ë ¨), ì„±ì  ì½˜í…ì¸ (`sexual`) ì¹´í…Œê³ ë¦¬ë¥¼ ì§€ì›í•œë‹¤. í…ìŠ¤íŠ¸ ì¤‘ì¬ì™€ ë§ˆì°¬ê°€ì§€ë¡œ ê° ì¹´í…Œê³ ë¦¬ë³„ ì„¸ë°€í•œ ì ìˆ˜ë¥¼ ì œê³µí•˜ì—¬ í”Œë«í¼ì˜ ì •ì±…ì— ë§ëŠ” ì„ê³„ê°’ ì„¤ì •ì´ ê°€ëŠ¥í•˜ë‹¤. íŠ¹íˆ ê·¸ë˜í”½í•œ í­ë ¥ ì½˜í…ì¸ ì™€ ì¼ë°˜ì ì¸ í­ë ¥ ì½˜í…ì¸ ë¥¼ êµ¬ë¶„í•˜ì—¬ ë¶„ë¥˜í•˜ë¯€ë¡œ, ì—°ë ¹ ì œí•œì´ë‚˜ ê²½ê³  í‘œì‹œ ë“± ì°¨ë“±ì ì¸ ì •ì±… ì ìš©ì´ ê°€ëŠ¥í•˜ë‹¤. ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ í•¨ê»˜ ë¶„ì„í•  ìˆ˜ ìˆì–´ ë§¥ë½ì„ ê³ ë ¤í•œ ë”ìš± ì •í™•í•œ ì¤‘ì¬ê°€ ê°€ëŠ¥í•˜ë©°, ì´ëŠ” ê¸°ì¡´ ì´ë¯¸ì§€ ë¶„ë¥˜ ì‹œìŠ¤í…œì˜ í•œê³„ë¥¼ í¬ê²Œ ê°œì„ í•œ ê²ƒì´ë‹¤.
```{python}
#| eval: false
import base64

# ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©
def encode_image(image_path):
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')

# ì´ë¯¸ì§€ ì¤‘ì¬
image_base64 = encode_image("images/gangnam_image.png")

response = client.moderations.create(
    input=[
        {
            "type": "image_url",
            "image_url": {
                "url": f"data:image/jpeg;base64,{image_base64}"
            }
        }
    ],
    model="omni-moderation-latest"
)

# ì´ë¯¸ì§€ ì¤‘ì¬ ê²°ê³¼ í™•ì¸
result = response.results[0]
print(f"ì´ë¯¸ì§€ ìœ„í—˜ í”Œë˜ê·¸: {result.flagged}")

# ì´ë¯¸ì§€ì—ì„œ ì§€ì›ë˜ëŠ” ì¹´í…Œê³ ë¦¬: violence, self-harm, sexual
supported_categories = ['violence', 'violence/graphic', 'self-harm', 'self-harm/intent', 
                       'self-harm/instructions', 'sexual']

for category in supported_categories:
    if category in result.category_scores:
        score = result.category_scores[category]
        if score > 0.01:
            print(f"ì´ë¯¸ì§€ {category}: {score:.4f}")
```

```markdown
ì´ë¯¸ì§€ ìœ„í—˜ í”Œë˜ê·¸: False
```

## ë¹„ìš© ìµœì í™”\index{ë¹„ìš© ìµœì í™”} íŒ

AI API ì‚¬ìš© ë¹„ìš©ì€ í”„ë¡œì íŠ¸ì˜ ì§€ì†ê°€ëŠ¥ì„±ì— ì§ì ‘ì ì¸ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ì¤‘ìš”í•œ ìš”ì†Œë‹¤. ì ì ˆí•œ ìµœì í™” ì „ëµì„ í†µí•´ ì„±ëŠ¥ ì €í•˜ ì—†ì´ ë¹„ìš©ì„ í¬ê²Œ ì ˆì•½í•  ìˆ˜ ìˆìœ¼ë©°, ì´ëŠ” íŠ¹íˆ ëŒ€ê·œëª¨ ì„œë¹„ìŠ¤ë‚˜ ì¥ê¸°ê°„ ìš´ì˜ë˜ëŠ” ì‹œìŠ¤í…œì—ì„œ ì¤‘ìš”í•˜ë‹¤. OpenAI API [@openai2025pricing]ì˜ ë¹„ìš©ì„ íš¨ìœ¨ì ìœ¼ë¡œ ê´€ë¦¬í•˜ê¸° ìœ„í•œ ë‹¤ì–‘í•œ ìµœì í™” ê¸°ë²•ì„ ì†Œê°œí•œë‹¤.

### ì ì ˆí•œ ëª¨ë¸ ì„ íƒ

ëª¨ë¸ ì„ íƒì€ ë¹„ìš© ìµœì í™”ì˜ ì²« ë²ˆì§¸ì´ì ê°€ì¥ ì¤‘ìš”í•œ ë‹¨ê³„ë‹¤. ì‘ì—…ì˜ ë³µì¡ì„±ê³¼ ìš”êµ¬ë˜ëŠ” í’ˆì§ˆ ìˆ˜ì¤€ì„ ì •í™•íˆ íŒŒì•…í•˜ì—¬ ê³¼ë„í•œ ì„±ëŠ¥ì˜ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì§€ ì•Šë„ë¡ ì£¼ì˜í•´ì•¼ í•œë‹¤. ëª¨ë¸ë³„ ì„±ëŠ¥ê³¼ ë¹„ìš©ì„ ê³ ë ¤í•˜ì—¬ ì‘ì—…ì— ë§ëŠ” ëª¨ë¸ì„ ì„ íƒí•œë‹¤.

```{python}
#| eval: false
# 2025ë…„ ê¸°ì¤€ ëª¨ë¸ë³„ ê°€ê²© ë° ìš©ë„
model_guide = {
    "gpt-4o": {
        "input": 0.0025,  # $/1K tokens
        "output": 0.01,
        "ìš©ë„": "ìµœê³  ì„±ëŠ¥ì´ í•„ìš”í•œ ë³µì¡í•œ ì‘ì—…, ë©€í‹°ëª¨ë‹¬",
        "ì˜ˆì‹œ": "ë³µì¡í•œ ë¶„ì„, ì½”ë“œ ìƒì„±, ì´ë¯¸ì§€ ì²˜ë¦¬"
    },
    "gpt-4o-mini": {
        "input": 0.00015,
        "output": 0.0006,
        "ìš©ë„": "ì¼ë°˜ì ì¸ ì‘ì—…, ê°€ì„±ë¹„ ìµœê³ ",
        "ì˜ˆì‹œ": "í…ìŠ¤íŠ¸ ìš”ì•½, ë²ˆì—­, ì¼ë°˜ ì§ˆë‹µ"
    },
    "gpt-3.5-turbo": {
        "input": 0.0005,
        "output": 0.0015,
        "ìš©ë„": "ê°„ë‹¨í•œ ì‘ì—…, ë¹ ë¥¸ ì‘ë‹µ",
        "ì˜ˆì‹œ": "ì±—ë´‡, ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ìƒì„±"
    },
    "o1-mini": {
        "input": 0.003,
        "output": 0.012,
        "ìš©ë„": "ë³µì¡í•œ ì¶”ë¡ , ìˆ˜í•™ ë¬¸ì œ",
        "ì˜ˆì‹œ": "ë…¼ë¦¬ì  ì¶”ë¡ , ë³µì¡í•œ ê³„ì‚°"
    }
}

def recommend_model(task_complexity, budget_priority=True):
    """ì‘ì—… ë³µì¡ë„ì™€ ì˜ˆì‚° ìš°ì„ ìˆœìœ„ì— ë”°ë¥¸ ëª¨ë¸ ì¶”ì²œ"""
    if task_complexity == "simple" and budget_priority:
        return "gpt-4o-mini"
    elif task_complexity == "simple":
        return "gpt-3.5-turbo"
    elif task_complexity == "complex" and budget_priority:
        return "gpt-4o-mini"
    elif task_complexity == "reasoning":
        return "o1-mini"
    else:
        return "gpt-4o"

# ì‚¬ìš© ì˜ˆì‹œ
print(f"ê°„ë‹¨í•œ ì‘ì—… (ì˜ˆì‚° ì¤‘ì‹œ): {recommend_model('simple', True)}")
print(f"ë³µì¡í•œ ì‘ì—… (ì„±ëŠ¥ ì¤‘ì‹œ): {recommend_model('complex', False)}")
```

```markdown
ê°„ë‹¨í•œ ì‘ì—… (ì˜ˆì‚° ì¤‘ì‹œ): gpt-4o-mini
ë³µì¡í•œ ì‘ì—… (ì„±ëŠ¥ ì¤‘ì‹œ): gpt-4o
```

### í† í° ì‚¬ìš©ëŸ‰ ìµœì í™”

í† í°ì€ API ë¹„ìš© ê³„ì‚°ì˜ ê¸°ë³¸ ë‹¨ìœ„ì´ë¯€ë¡œ ì •í™•í•œ ê³„ì‚°ê³¼ ì˜ˆì¸¡ì´ í•„ìˆ˜ì ì´ë‹¤. ì˜ˆìƒì¹˜ ëª»í•œ ë¹„ìš© ì´ˆê³¼ë¥¼ ë°©ì§€í•˜ê³  í”„ë¡œì íŠ¸ ì˜ˆì‚°ì„ íš¨ê³¼ì ìœ¼ë¡œ ê´€ë¦¬í•  ìˆ˜ ìˆë‹¤. OpenAI Tokenizer [@openai2025tokenizer]ë¥¼ í™œìš©í•˜ì—¬ ì •í™•í•œ í† í° ìˆ˜ë¥¼ ê³„ì‚°í•˜ê³  ë¹„ìš©ì„ ì˜ˆì¸¡í•œë‹¤.

```{python}
#| eval: false
import tiktoken

def count_tokens(text, model="gpt-4o-mini"):
    """ì •í™•í•œ í† í° ìˆ˜ ê³„ì‚°"""
    try:
        encoding = tiktoken.encoding_for_model(model)
    except KeyError:
        encoding = tiktoken.get_encoding("cl100k_base")  # ê¸°ë³¸ ì¸ì½”ë”©
    return len(encoding.encode(text))

def estimate_cost(prompt, response="", model="gpt-4o-mini"):
    """API í˜¸ì¶œ ë¹„ìš© ì¶”ì •"""
    model_pricing = {
        "gpt-4o-mini": {"input": 0.00015, "output": 0.0006},
        "gpt-4o": {"input": 0.0025, "output": 0.01},
        "gpt-3.5-turbo": {"input": 0.0005, "output": 0.0015}
    }
    
    input_tokens = count_tokens(prompt, model)
    output_tokens = count_tokens(response, model) if response else 0
    
    pricing = model_pricing.get(model, model_pricing["gpt-4o-mini"])
    
    input_cost = (input_tokens / 1000) * pricing["input"]
    output_cost = (output_tokens / 1000) * pricing["output"]
    total_cost = input_cost + output_cost
    
    return {
        "input_tokens": input_tokens,
        "output_tokens": output_tokens,
        "input_cost": input_cost,
        "output_cost": output_cost,
        "total_cost": total_cost
    }

# í”„ë¡¬í”„íŠ¸ ìµœì í™” ì˜ˆì‹œ
verbose_prompt = """
ì•ˆë…•í•˜ì„¸ìš”. ì €ëŠ” ë°ì´í„° ë¶„ì„ì„ ê³µë¶€í•˜ê³  ìˆëŠ” í•™ìƒì…ë‹ˆë‹¤. 
íŒŒì´ì¬ê³¼ R ì–¸ì–´ ì¤‘ì—ì„œ ì–´ë–¤ ê²ƒì´ ë” ì¢‹ì€ì§€ ë§¤ìš° ìì„¸í•˜ê³  
êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤.
"""

optimized_prompt = "íŒŒì´ì¬ vs R: ë°ì´í„° ë¶„ì„ìš© ì–¸ì–´ ë¹„êµ (ì¥ë‹¨ì , ì‚¬ìš© ì‚¬ë¡€)"

print("=== í† í° ë° ë¹„ìš© ë¹„êµ ===")
verbose_cost = estimate_cost(verbose_prompt)
optimized_cost = estimate_cost(optimized_prompt)

print(f"ì¥í™©í•œ í”„ë¡¬í”„íŠ¸: {verbose_cost['input_tokens']} í† í°, ${verbose_cost['input_cost']:.6f}")
print(f"ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸: {optimized_cost['input_tokens']} í† í°, ${optimized_cost['input_cost']:.6f}")
print(f"ì ˆì•½: {verbose_cost['input_tokens'] - optimized_cost['input_tokens']} í† í°")
```

```markdown
=== í† í° ë° ë¹„ìš© ë¹„êµ ===
ì¥í™©í•œ í”„ë¡¬í”„íŠ¸: 45 í† í°, $0.000007
ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸: 21 í† í°, $0.000003
ì ˆì•½: 24 í† í°
```

### í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§

íš¨ê³¼ì ì¸ í”„ë¡¬í”„íŠ¸ëŠ” ë” ì ì€ í† í°ìœ¼ë¡œ ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ì–»ëŠ” í•µì‹¬ ë°©ë²•ì´ë‹¤. ëª…í™•í•˜ê³  êµ¬ì¡°í™”ëœ í”„ë¡¬í”„íŠ¸ëŠ” AIê°€ ì˜ë„ë¥¼ ì •í™•íˆ íŒŒì•…í•˜ê²Œ í•˜ì—¬ ë¶ˆí•„ìš”í•œ ì¬ì‹œë„ë¥¼ ì¤„ì¸ë‹¤.

```{python}
#| eval: false
# ë¹„íš¨ìœ¨ì ì¸ í”„ë¡¬í”„íŠ¸
inefficient_prompt = """
ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ì½ê³  ë§¤ìš° ìì„¸í•˜ê²Œ ë¶„ì„í•´ì£¼ì„¸ìš”. 
ëª¨ë“  ì¸¡ë©´ì„ ê³ ë ¤í•˜ì—¬ ì™„ì „í•˜ê³  í¬ê´„ì ì¸ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.
ê°€ëŠ¥í•œ í•œ ë§ì€ ì •ë³´ë¥¼ í¬í•¨í•˜ì—¬ ì„¤ëª…í•´ì£¼ì„¸ìš”.

í…ìŠ¤íŠ¸: "ë¨¸ì‹ ëŸ¬ë‹ì€ ì¸ê³µì§€ëŠ¥ì˜ í•œ ë¶„ì•¼ì…ë‹ˆë‹¤."
"""

# íš¨ìœ¨ì ì¸ í”„ë¡¬í”„íŠ¸
efficient_prompt = """
ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ 3ì¤„ë¡œ ìš”ì•½í•˜ì„¸ìš”:
"ë¨¸ì‹ ëŸ¬ë‹ì€ ì¸ê³µì§€ëŠ¥ì˜ í•œ ë¶„ì•¼ì…ë‹ˆë‹¤."

í˜•ì‹:
1. ì •ì˜
2. íŠ¹ì§•  
3. ì‘ìš©ë¶„ì•¼
"""

def create_structured_prompt(task, content, output_format=None, examples=None):
    """êµ¬ì¡°í™”ëœ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
    prompt_parts = [f"ì‘ì—…: {task}"]
    
    if content:
        prompt_parts.append(f"ë‚´ìš©: {content}")
    
    if output_format:
        prompt_parts.append(f"ì¶œë ¥ í˜•ì‹: {output_format}")
    
    if examples:
        prompt_parts.append(f"ì˜ˆì‹œ: {examples}")
    
    return "\n\n".join(prompt_parts)

# êµ¬ì¡°í™”ëœ í”„ë¡¬í”„íŠ¸ ì˜ˆì‹œ
structured = create_structured_prompt(
    task="í…ìŠ¤íŠ¸ ê°ì • ë¶„ì„",
    content="ì´ ì˜í™” ì •ë§ ì¬ë¯¸ìˆì—ˆì–´ìš”!",
    output_format="JSON {sentiment: positive/negative/neutral, confidence: 0-1}",
    examples='{"sentiment": "positive", "confidence": 0.9}'
)

print("=== êµ¬ì¡°í™”ëœ í”„ë¡¬í”„íŠ¸ ===")
print(structured)
```

```markdown
=== êµ¬ì¡°í™”ëœ í”„ë¡¬í”„íŠ¸ ===
ì‘ì—…: í…ìŠ¤íŠ¸ ê°ì • ë¶„ì„

ë‚´ìš©: ì´ ì˜í™” ì •ë§ ì¬ë¯¸ìˆì—ˆì–´ìš”!

ì¶œë ¥ í˜•ì‹: JSON {sentiment: positive/negative/neutral, confidence: 0-1}

ì˜ˆì‹œ: {"sentiment": "positive", "confidence": 0.9}
```

### ì‘ë‹µ ê¸¸ì´ ì œí•œ

ì¶œë ¥ í† í° ìˆ˜ë¥¼ ì œí•œí•˜ëŠ” ê²ƒì€ ì˜ˆì‚°ì„ ì—„ê²©íˆ ê´€ë¦¬í•˜ëŠ” íš¨ê³¼ì ì¸ ë°©ë²•ì´ë‹¤. íŠ¹íˆ ì‹¤ì‹œê°„ ì„œë¹„ìŠ¤ì—ì„œ ì˜ˆì¸¡ ê°€ëŠ¥í•œ ë¹„ìš© êµ¬ì¡°ë¥¼ ë§Œë“¤ ë•Œ ìœ ìš©í•˜ë‹¤.

```{python}
#| eval: false
# max_tokensì„ í™œìš©í•œ ë¹„ìš© ì œì–´
def controlled_generation(prompt, max_budget_usd=0.01, model="gpt-4o-mini"):
    """ì˜ˆì‚° ë‚´ì—ì„œ ì‘ë‹µ ìƒì„±"""
    model_pricing = {
        "gpt-4o-mini": {"input": 0.00015, "output": 0.0006},
        "gpt-4o": {"input": 0.0025, "output": 0.01}
    }
    
    input_tokens = count_tokens(prompt, model)
    input_cost = (input_tokens / 1000) * model_pricing[model]["input"]
    
    # ë‚¨ì€ ì˜ˆì‚°ìœ¼ë¡œ ìƒì„± ê°€ëŠ¥í•œ ìµœëŒ€ í† í° ìˆ˜ ê³„ì‚°
    remaining_budget = max_budget_usd - input_cost
    max_output_tokens = int((remaining_budget / model_pricing[model]["output"]) * 1000)
    
    if max_output_tokens <= 0:
        return {"error": "ì…ë ¥ ë¹„ìš©ì´ ì˜ˆì‚°ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤."}
    
    # ì‹¤ì œ API í˜¸ì¶œ (ì˜ˆì‹œ)
    response = client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": prompt}],
        max_tokens=min(max_output_tokens, 1000)  # ì•ˆì „ ì œí•œ
    )
    
    return {
        "response": response.choices[0].message.content,
        "input_tokens": input_tokens,
        "estimated_output_tokens": max_output_tokens,
        "estimated_cost": max_budget_usd
    }
```

### ìºì‹± ë° ì¬ì‚¬ìš©

ë™ì¼í•˜ê±°ë‚˜ ìœ ì‚¬í•œ ìš”ì²­ì´ ë°˜ë³µë˜ëŠ” í™˜ê²½ì—ì„œëŠ” ìºì‹±ì„ í†µí•´ API í˜¸ì¶œ íšŸìˆ˜ë¥¼ í¬ê²Œ ì¤„ì¼ ìˆ˜ ìˆë‹¤. ì´ëŠ” ë¹„ìš© ì ˆì•½ë¿ë§Œ ì•„ë‹ˆë¼ ì‘ë‹µ ì†ë„ í–¥ìƒì—ë„ ê¸°ì—¬í•œë‹¤.

```{python}
#| eval: false
import hashlib
import json
from functools import lru_cache

class APICache:
    """API ì‘ë‹µ ìºì‹± í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.cache = {}
    
    def _generate_key(self, prompt, model, temperature=0):
        """ìºì‹œ í‚¤ ìƒì„±"""
        cache_data = {
            "prompt": prompt,
            "model": model, 
            "temperature": temperature
        }
        return hashlib.md5(json.dumps(cache_data, sort_keys=True).encode()).hexdigest()
    
    def get_response(self, prompt, model="gpt-4o-mini", temperature=0):
        """ìºì‹œëœ ì‘ë‹µ ì¡°íšŒ ë˜ëŠ” ìƒˆë¡œìš´ API í˜¸ì¶œ"""
        cache_key = self._generate_key(prompt, model, temperature)
        
        if cache_key in self.cache:
            print("ğŸ“‹ ìºì‹œì—ì„œ ì‘ë‹µ ë°˜í™˜")
            return self.cache[cache_key]
        
        print("ğŸŒ ìƒˆë¡œìš´ API í˜¸ì¶œ")
        response = client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            temperature=temperature
        )
        
        result = response.choices[0].message.content
        self.cache[cache_key] = result
        return result

# ì‚¬ìš© ì˜ˆì‹œ
cache = APICache()

# ì²« ë²ˆì§¸ í˜¸ì¶œ - API ì‚¬ìš©
response1 = cache.get_response("Pythonì˜ ì¥ì  3ê°€ì§€")

# ë‘ ë²ˆì§¸ ë™ì¼ í˜¸ì¶œ - ìºì‹œ ì‚¬ìš©
response2 = cache.get_response("Pythonì˜ ì¥ì  3ê°€ì§€")
```


```markdown
ğŸŒ ìƒˆë¡œìš´ API í˜¸ì¶œ
ğŸ“‹ ìºì‹œì—ì„œ ì‘ë‹µ ë°˜í™˜
```

### ë°°ì¹˜ ì²˜ë¦¬

ì—¬ëŸ¬ ê°œì˜ ê°œë³„ ìš”ì²­ ëŒ€ì‹  í•˜ë‚˜ì˜ ë°°ì¹˜ ìš”ì²­ìœ¼ë¡œ ì²˜ë¦¬í•˜ë©´ ì´ í† í° ìˆ˜ë¥¼ ì¤„ì´ê³  API í˜¸ì¶œ ì˜¤ë²„í—¤ë“œë¥¼ ìµœì†Œí™”í•  ìˆ˜ ìˆë‹¤. íŠ¹íˆ ëŒ€ëŸ‰ì˜ í…ìŠ¤íŠ¸ë¥¼ ì²˜ë¦¬í•  ë•Œ íš¨ê³¼ì ì´ë‹¤.

```{python}
#| eval: false
def batch_process_texts(texts, instruction, model="gpt-4o-mini"):
    """ì—¬ëŸ¬ í…ìŠ¤íŠ¸ë¥¼ í•œ ë²ˆì— ì²˜ë¦¬í•˜ì—¬ ë¹„ìš© ì ˆì•½"""
    
    # ê°œë³„ ì²˜ë¦¬ vs ë°°ì¹˜ ì²˜ë¦¬ ë¹„êµ
    individual_cost = 0
    for text in texts:
        prompt = f"{instruction}\n\ní…ìŠ¤íŠ¸: {text}"
        cost = estimate_cost(prompt, model=model)
        individual_cost += cost['total_cost']
    
    # ë°°ì¹˜ ì²˜ë¦¬
    batch_prompt = f"{instruction}\n\n"
    for i, text in enumerate(texts, 1):
        batch_prompt += f"{i}. {text}\n"
    
    batch_cost = estimate_cost(batch_prompt, model=model)
    
    print(f"ê°œë³„ ì²˜ë¦¬ ì˜ˆìƒ ë¹„ìš©: ${individual_cost:.6f}")
    print(f"ë°°ì¹˜ ì²˜ë¦¬ ì˜ˆìƒ ë¹„ìš©: ${batch_cost['total_cost']:.6f}")
    print(f"ì ˆì•½ë¥ : {((individual_cost - batch_cost['total_cost']) / individual_cost) * 100:.1f}%")
    
    # ì‹¤ì œ ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰
    response = client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": batch_prompt}]
    )
    
    return response.choices[0].message.content

# ì‚¬ìš© ì˜ˆì‹œ
texts_to_analyze = [
    "ì´ ì œí’ˆì€ ì •ë§ í›Œë¥­í•©ë‹ˆë‹¤!",
    "ë°°ì†¡ì´ ë„ˆë¬´ ëŠ¦ì—ˆì–´ìš”.",
    "ê°€ê²© ëŒ€ë¹„ ê´œì°®ì€ ê²ƒ ê°™ì•„ìš”.",
    "ê³ ê° ì„œë¹„ìŠ¤ê°€ ë³„ë¡œì˜€ìŠµë‹ˆë‹¤."
]

batch_result = batch_process_texts(
    texts_to_analyze, 
    "ë‹¤ìŒ í…ìŠ¤íŠ¸ë“¤ì˜ ê°ì •ì„ positive/negative/neutralë¡œ ë¶„ë¥˜í•˜ì„¸ìš”:"
)
```


```markdown
ê°œë³„ ì²˜ë¦¬ ì˜ˆìƒ ë¹„ìš©: $0.000018
ë°°ì¹˜ ì²˜ë¦¬ ì˜ˆìƒ ë¹„ìš©: $0.000009
ì ˆì•½ë¥ : 48.7%
```

### ìŠ¤íŠ¸ë¦¬ë° ì¡°ê¸° ì¢…ë£Œ

ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì—ì„œ í•„ìš”í•œ ì •ë³´ë¥¼ ì–»ëŠ” ì¦‰ì‹œ ì—°ê²°ì„ ì¢…ë£Œí•˜ë©´ ë¶ˆí•„ìš”í•œ í† í° ìƒì„±ì„ ë°©ì§€í•  ìˆ˜ ìˆë‹¤. ìš”ì•½ì´ë‚˜ í•µì‹¬ ì •ë³´ ì¶”ì¶œ ì‘ì—…ì—ì„œ íŠ¹íˆ ìœ ìš©í•˜ë‹¤.

```{python}
#| eval: false
def streaming_with_early_stop(prompt, stop_conditions=None, model="gpt-4o-mini"):
    """ìŠ¤íŠ¸ë¦¬ë°ì„ í™œìš©í•œ ì¡°ê¸° ì¢…ë£Œë¡œ ë¹„ìš© ì ˆì•½"""
    
    if stop_conditions is None:
        stop_conditions = ["ê²°ë¡ ì ìœ¼ë¡œ", "ìš”ì•½í•˜ë©´", "ë§ˆì§€ë§‰ìœ¼ë¡œ"]
    
    response_text = ""
    
    stream = client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": prompt}],
        stream=True,
        max_tokens=500
    )
    
    for chunk in stream:
        if chunk.choices[0].delta.content:
            content = chunk.choices[0].delta.content
            response_text += content
            print(content, end="")
            
            # ì¡°ê¸° ì¢…ë£Œ ì¡°ê±´ í™•ì¸
            for condition in stop_conditions:
                if condition in response_text:
                    print(f"\n\n[ì¡°ê¸° ì¢…ë£Œ: '{condition}' ê°ì§€]")
                    return response_text
    
    return response_text

# ì‚¬ìš© ì˜ˆì‹œ
prompt = "ë¨¸ì‹ ëŸ¬ë‹ì˜ ì¢…ë¥˜ì™€ íŠ¹ì§•ì— ëŒ€í•´ ê°„ëµíˆ ì„¤ëª…í•´ì£¼ì„¸ìš”."
result = streaming_with_early_stop(prompt)
```

### ë¹„ìš© ëª¨ë‹ˆí„°ë§

ì‹¤ì‹œê°„ ë¹„ìš© ì¶”ì ì€ ì˜ˆì‚° ì´ˆê³¼ë¥¼ ë°©ì§€í•˜ê³  ì‚¬ìš© íŒ¨í„´ì„ ë¶„ì„í•˜ëŠ” ë° í•„ìˆ˜ì ì´ë‹¤. ì²´ê³„ì ì¸ ëª¨ë‹ˆí„°ë§ì„ í†µí•´ ìµœì í™” í¬ì¸íŠ¸ë¥¼ ì°¾ì•„ë‚¼ ìˆ˜ ìˆë‹¤.

```{python}
#| eval: false
class CostMonitor:
    """API ì‚¬ìš©ëŸ‰ ë° ë¹„ìš© ëª¨ë‹ˆí„°ë§"""
    
    def __init__(self, daily_budget=1.0):
        self.daily_budget = daily_budget
        self.daily_usage = 0.0
        self.call_history = []
    
    def track_call(self, model, input_tokens, output_tokens):
        """API í˜¸ì¶œ ì¶”ì """
        model_pricing = {
            "gpt-4o-mini": {"input": 0.00015, "output": 0.0006},
            "gpt-4o": {"input": 0.0025, "output": 0.01}
        }
        
        pricing = model_pricing.get(model, model_pricing["gpt-4o-mini"])
        cost = (input_tokens/1000 * pricing["input"]) + (output_tokens/1000 * pricing["output"])
        
        self.daily_usage += cost
        self.call_history.append({
            "model": model,
            "input_tokens": input_tokens,
            "output_tokens": output_tokens,
            "cost": cost,
            "cumulative": self.daily_usage
        })
        
        return self.check_budget()
    
    def check_budget(self):
        """ì˜ˆì‚° í™•ì¸"""
        remaining = self.daily_budget - self.daily_usage
        percentage = (self.daily_usage / self.daily_budget) * 100
        
        status = {
            "current_usage": self.daily_usage,
            "daily_budget": self.daily_budget,
            "remaining": remaining,
            "percentage": percentage,
            "warning": percentage > 80,
            "exceeded": remaining < 0
        }
        
        if status["warning"]:
            print(f"âš ï¸ ì˜ˆì‚°ì˜ {percentage:.1f}% ì‚¬ìš©ë¨")
        
        if status["exceeded"]:
            print(f"ğŸš« ì¼ì¼ ì˜ˆì‚° ì´ˆê³¼! (${self.daily_usage:.4f}/${self.daily_budget})")
        
        return status

# ì‚¬ìš© ì˜ˆì‹œ
monitor = CostMonitor(daily_budget=5.0)

# API í˜¸ì¶œ ì‹œë®¬ë ˆì´ì…˜
status = monitor.track_call("gpt-4o-mini", 100, 200)
print(f"í˜„ì¬ ì‚¬ìš©ëŸ‰: ${status['current_usage']:.4f}")
print(f"ë‚¨ì€ ì˜ˆì‚°: ${status['remaining']:.4f}")
```

```markdown
í˜„ì¬ ì‚¬ìš©ëŸ‰: $0.0001
ë‚¨ì€ ì˜ˆì‚°: $4.9999
```

### ì„±ëŠ¥ ëŒ€ ë¹„ìš© ë²¤ì¹˜ë§ˆí¬

ì •ê¸°ì ì¸ ë²¤ì¹˜ë§ˆí‚¹ì„ í†µí•´ ê° ëª¨ë¸ì˜ ê°€ì„±ë¹„ë¥¼ í‰ê°€í•˜ê³ , ìš”êµ¬ì‚¬í•­ ë³€í™”ì— ë”°ë¼ ëª¨ë¸ ì„ íƒì„ ì¡°ì •í•  ìˆ˜ ìˆë‹¤. ë°ì´í„° ê¸°ë°˜ ì˜ì‚¬ê²°ì •ìœ¼ë¡œ ìµœì ì˜ ëª¨ë¸ ì¡°í•©ì„ ì°¾ëŠ”ë‹¤.

```{python}
#| eval: false
def benchmark_models(test_prompts, models=None):
    """ëª¨ë¸ë³„ ì„±ëŠ¥ ëŒ€ë¹„ ë¹„ìš© ë¶„ì„"""
    
    if models is None:
        models = ["gpt-4o-mini", "gpt-3.5-turbo", "gpt-4o"]
    
    results = []
    
    for model in models:
        total_cost = 0
        total_quality = 0
        
        for prompt in test_prompts:
            cost_estimate = estimate_cost(prompt, model=model)
            
            # ì‹¤ì œë¡œëŠ” ì‘ë‹µ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” ë¡œì§ì´ í•„ìš”
            # ì—¬ê¸°ì„œëŠ” ì˜ˆì‹œë¡œ ê°„ë‹¨í•œ ì ìˆ˜ ì‚¬ìš©
            quality_score = {
                "gpt-4o": 9.5,
                "gpt-4o-mini": 8.5, 
                "gpt-3.5-turbo": 7.5
            }.get(model, 7.0)
            
            total_cost += cost_estimate['input_cost']
            total_quality += quality_score
        
        avg_cost = total_cost / len(test_prompts)
        avg_quality = total_quality / len(test_prompts)
        value_score = avg_quality / (avg_cost * 1000)  # ê°€ì„±ë¹„ ì ìˆ˜
        
        results.append({
            "model": model,
            "avg_cost_per_prompt": avg_cost,
            "avg_quality": avg_quality,
            "value_score": value_score
        })
    
    # ê²°ê³¼ ì •ë ¬ (ê°€ì„±ë¹„ ìˆœ)
    results.sort(key=lambda x: x['value_score'], reverse=True)
    
    print("=== ëª¨ë¸ ì„±ëŠ¥ ëŒ€ë¹„ ë¹„ìš© ë¶„ì„ ===")
    for result in results:
        print(f"{result['model']}: í’ˆì§ˆ {result['avg_quality']:.1f}, "
              f"ë¹„ìš© ${result['avg_cost_per_prompt']:.6f}, "
              f"ê°€ì„±ë¹„ {result['value_score']:.1f}")
    
    return results

# í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
test_prompts = [
    "Python ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ ì„¤ëª…",
    "ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì˜ ì°¨ì´ì ", 
    "API ì„¤ê³„ ì›ì¹™ 3ê°€ì§€"
]

benchmark_results = benchmark_models(test_prompts)
```

```markdown
=== ëª¨ë¸ ì„±ëŠ¥ ëŒ€ë¹„ ë¹„ìš© ë¶„ì„ ===
gpt-4o-mini: í’ˆì§ˆ 8.5, ë¹„ìš© $0.000001, ê°€ì„±ë¹„ 5666.7
gpt-3.5-turbo: í’ˆì§ˆ 7.5, ë¹„ìš© $0.000007, ê°€ì„±ë¹„ 1022.7
gpt-4o: í’ˆì§ˆ 9.5, ë¹„ìš© $0.000025, ê°€ì„±ë¹„ 380.0
```

ì´ëŸ¬í•œ ë¹„ìš© ìµœì í™” ê¸°ë²•ë“¤ì„ ì²´ê³„ì ìœ¼ë¡œ ì¡°í•©í•˜ì—¬ ì‚¬ìš©í•˜ë©´ ì„±ëŠ¥ ì €í•˜ ì—†ì´ OpenAI API ë¹„ìš©ì„ í¬ê²Œ ì ˆì•½í•  ìˆ˜ ìˆìœ¼ë©°, ì§€ì†ê°€ëŠ¥í•œ AI ì„œë¹„ìŠ¤ ê°œë°œì´ ê°€ëŠ¥í•˜ë‹¤.


