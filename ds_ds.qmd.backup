# 데이터 과학 {#sec-data-science}

데이터 과학(Data Science)은 데이터에서 의미 있는 인사이트를 추출하고 의사결정을 지원하는 학문 분야다. 통계학, 컴퓨터 과학, 도메인 지식이 결합된 융합 학문으로, 현대 조직의 핵심 역량이 되었다. 본 장에서는 데이터 기반 의사결정의 전통적 프로세스와 AI 시대의 변화를 살펴본다.

## 데이터 기반 의사결정 연속체 {#sec-decision-continuum}

전통적인 데이터 기반 의사결정 프로세스는 **7단계 연속체(Continuum)**로 구성된다. 이는 인간 중심의 선형적 프로세스로, 각 단계가 순차적으로 진행되며 전체 사이클 완료에 2-6개월이 소요된다.

![전통적 데이터 기반 의사결정 연속체](images/ds-continuum-traditional.svg){#fig-continuum-traditional fig-align="center"}

@fig-continuum-traditional 는 7단계 프로세스를 시각화한다. 각 단계는 고유한 목적과 소요 시간을 가진다.

### 1단계: 획득(Acquire) {.unnumbered}

데이터 수집 단계로, 데이터베이스 쿼리, API 연결, 파일 임포트, 설문 조사, 수동 입력 등 다양한 방식으로 데이터를 확보한다. 일반적으로 **수일에서 수주**가 소요되며, 데이터 품질과 접근성에 따라 기간이 크게 달라진다.

### 2단계: 통합(Integrate) {.unnumbered}

수집된 데이터를 결합하고 정제하는 단계다. 데이터 클리닝, ETL(Extract, Transform, Load) 프로세스, 스키마 매핑, 품질 검사, 중복 제거 작업을 수행한다. **수일에서 수주**가 소요되며, 데이터 소스의 이질성이 클수록 복잡도가 증가한다.

### 3단계: 노출(Surface) {.unnumbered}

통합된 데이터를 조직 구성원이 접근할 수 있도록 만드는 단계다. 대시보드 생성, 쿼리 인터페이스, 데이터 탐색 도구, 시각화 도구, 접근 권한 관리를 포함한다. **수 시간에서 수일**이 소요되며, 기술 스택과 요구사항에 따라 달라진다.

### 4단계: 보고(Report) {.unnumbered}

데이터를 분석하고 문서화하는 단계다. 통계 분석, 추세 식별, KPI 계산, 성과 지표, 비교 분석을 수행하여 객관적 사실을 도출한다. **수 시간에서 수일**이 소요되며, 분석의 깊이와 범위에 따라 결정된다.

### 5단계: 스토리텔링(Storytell) {.unnumbered}

분석 결과를 맥락 속에 배치하고 서사를 구성하는 단계다. 컨텍스트 구축, 인사이트 종합, 프레젠테이션 디자인, 내러티브 구조, 권고사항 프레이밍을 통해 데이터에 의미를 부여한다. **수 시간에서 수일**이 소요되며, 이해관계자의 특성에 맞춘 커뮤니케이션이 핵심이다.

### 6단계: 결정(Decide) {.unnumbered}

스토리에 기반하여 의사결정을 내리는 단계다. 옵션 평가, 위험 평가, 이해관계자 의견 수렴, 비용-편익 분석, 최종 선택을 거쳐 결론에 도달한다. **수일에서 수주**가 소요되며, 의사결정의 중요도와 조직 구조에 따라 기간이 결정된다.

### 7단계: 실행(Take Action) {.unnumbered}

결정사항을 실제로 적용하는 단계다. 자원 배분, 프로세스 변경, 시스템 업데이트, 팀 조정, 결과 모니터링을 통해 의사결정을 현실화한다. **수일에서 수개월**이 소요되며, 변화의 규모와 조직의 민첩성에 따라 달라진다.

## 전통적 프로세스의 한계 {#sec-traditional-limitations}

이러한 선형적 7단계 프로세스는 체계적이고 검증 가능하다는 장점이 있지만, 현대 비즈니스 환경에서 심각한 한계를 드러낸다.

**시간 지연 문제**가 가장 크다. 전체 사이클 완료까지 2-6개월이 소요되는 동안 비즈니스 환경은 이미 변화한다. 특히 전자상거래, 금융, 소셜미디어 같은 빠르게 변하는 산업에서는 의사결정이 완료되는 시점에 이미 기회를 놓치거나 위험이 현실화된다.

**단절된 워크플로우**도 문제다. 각 단계가 서로 다른 팀과 도구로 분리되어 있어, 커뮤니케이션 비용이 증가하고 정보 손실이 발생한다. 데이터 엔지니어가 수집한 데이터를 분석가가 이해하는 데 시간이 걸리고, 분석가의 인사이트를 의사결정권자가 해석하는 데 또 다른 노력이 필요하다.

**인간 의존성**은 확장성을 제한한다. 모든 단계가 인간의 수동 개입을 필요로 하므로, 처리할 수 있는 의사결정의 수와 속도에 본질적 한계가 있다. 대규모 조직에서 수백 개의 동시다발적 의사결정을 지원하기 위해서는 그에 비례하는 인력이 필요하다.

## AI 시대의 변화 {#sec-ai-transformation}

AI, 특히 대규모 언어 모델(LLM)의 등장은 이러한 전통적 프로세스를 근본적으로 변화시키고 있다. ChatGPT, Claude 같은 도구들은 **7단계를 하나의 대화형 인터페이스로 통합**하여, 수개월 걸리던 프로세스를 수분으로 단축시킨다.

**자연어 인터페이스**를 통해 비기술 사용자도 복잡한 데이터 분석을 수행할 수 있다. "지난 분기 매출 상위 10% 고객의 특징을 분석하고 타겟 마케팅 전략을 제안해줘"라는 하나의 질문으로, 데이터 획득부터 실행 방안까지 전체 연속체를 순식간에 처리한다.

**실시간 반복**이 가능해진다. 전통적 프로세스에서는 각 단계가 완료되어야 다음 단계로 넘어갈 수 있었지만, AI는 즉각적인 피드백과 수정을 통해 탐색적 분석을 지원한다. "이번엔 고객 연령대별로 세분화해줘" 같은 후속 질문으로 분석을 즉시 확장하거나 변경할 수 있다.

**확장성과 민주화**가 실현된다. 한정된 데이터 과학 인력으로는 소수의 중요한 의사결정만 지원할 수 있었지만, AI는 모든 조직 구성원이 데이터 기반 의사결정을 일상적으로 수행할 수 있게 해준다. 현장 직원부터 경영진까지 각자의 맥락에서 데이터를 활용할 수 있다.

그러나 AI 시대에도 전통적 7단계 연속체의 개념적 프레임워크는 여전히 유효하다. AI는 프로세스를 자동화하고 가속화할 뿐, 데이터 기반 의사결정의 본질적 구조를 대체하지는 않는다. 오히려 각 단계의 목적과 중요성을 이해하는 것이 AI 도구를 효과적으로 활용하는 전제 조건이 된다.

## DIKW 계층 구조 {#sec-dikw-hierarchy}

데이터 과학의 핵심은 **원시 데이터(Raw Data)**를 **실행 가능한 지혜(Actionable Wisdom)**로 변환하는 과정이다. 이를 이해하기 위한 가장 영향력 있는 프레임워크가 러셀 애코프(Russell Ackoff)가 1989년 제시한 **DIKW 계층 구조(DIKW Hierarchy)**다.

![DIKW 계층 구조: 데이터에서 지혜로의 변환](images/ds-continuum-dikw-hierarchy.svg){#fig-dikw-hierarchy fig-align="center"}

@fig-dikw-hierarchy 는 데이터, 정보, 지식, 지혜의 4단계 계층과 각 단계 간 변환 과정을 시각화한다. 피라미드 구조는 각 단계로 올라갈수록 **볼륨은 감소하지만 가치와 복잡도는 증가**한다는 핵심 특성을 나타낸다.

### 데이터(Data): 가공되지 않은 사실 {.unnumbered}

가장 하위 계층인 **데이터**는 숫자, 기호, 관찰 결과 등 가공되지 않은 원시 사실(Raw Facts)이다. 그 자체로는 의미나 해석이 없으며, 저장, 전송, 처리가 가능한 기본 단위다. 예를 들어 "38.5", "John", "2024-01-15" 같은 개별 값들이 데이터다.

### 정보(Information): 맥락이 부여된 데이터 {.unnumbered}

**정보**는 데이터에 맥락, 관련성, 목적이 부여된 상태다. "What? When? Where?"에 답하며, 불확실성을 감소시키고 커뮤니케이션을 가능하게 한다. 예를 들어 "환자의 체온이 38.5°C로 아침 진찰 시 발열 상태"라는 문장은 원시 데이터에 시간적, 의학적 맥락을 부여한 정보다.

### 지식(Knowledge): 경험을 통한 이해 {.unnumbered}

**지식**은 정보와 경험, 학습이 결합되어 형성된 이해(Understanding)다. "How? What if?"에 답하며, 문제 해결과 예측을 가능하게 한다. 예를 들어 "이러한 증상 패턴은 임상 경험상 바이러스 감염을 시사한다"는 진술은 개별 정보를 넘어 전문가의 경험과 학습을 통해 축적된 지식이다.

### 지혜(Wisdom): 가치 기반 판단 {.unnumbered}

최상위 계층인 **지혜**는 지식에 가치(Values), 윤리(Ethics), 판단력(Judgment)이 결합된 상태다. "Why should we? What ought?"에 답하며, 장기적 결과를 고려한 건전한 의사결정을 가능하게 한다. 예를 들어 "환자 복지를 고려할 때, 항생제 처방보다는 휴식과 경과 관찰을 권장한다"는 판단은 의학 지식에 환자 중심 가치와 윤리적 고려를 결합한 지혜다.

### DIKW 계층의 현대적 의의 {.unnumbered}

이 프레임워크는 1989년 제시되었지만, AI 시대에 오히려 더욱 중요해졌다. 현대 조직은 **데이터 과잉(Data Overload)** 상황에 직면해 있다. 센서, 로그, 트랜잭션 등에서 생성되는 방대한 원시 데이터는 저장과 처리는 쉬워졌지만, 이를 의미 있는 정보로, 다시 실행 가능한 지식과 지혜로 변환하는 것은 여전히 어렵다.

AI, 특히 대규모 언어 모델의 진정한 가치는 **DIKW 변환 과정의 자동화와 가속화**에 있다. ChatGPT나 Claude 같은 도구는:

- **데이터 → 정보**: 원시 데이터를 읽고 맥락을 파악하여 의미 있는 정보로 구조화
- **정보 → 지식**: 패턴을 인식하고 방대한 학습 데이터를 바탕으로 이해와 예측 제공
- **지식 → 지혜**: 다양한 관점과 윤리적 고려사항을 통합하여 균형 잡힌 판단 지원

그러나 중요한 것은, AI가 각 계층을 완전히 대체하는 것이 아니라 **인간의 DIKW 상승을 지원하는 도구**라는 점이다. 최종적인 가치 판단과 윤리적 의사결정은 여전히 인간의 영역이며, AI는 그 과정을 가속화하고 확장하는 역할을 한다.

다음 장에서는 AI 시대 데이터 과학자의 역할과 필요한 역량을 구체적으로 살펴본다.

## DIKW를 넘어서: AI 지능 프레임워크 {#sec-beyond-dikw}

DIKW 계층 구조는 1989년 이후 데이터 과학의 표준 프레임워크로 자리 잡았지만, 2025년 현재 AI 시대의 현실을 완전히 설명하기에는 한계가 있다. 전통적 DIKW는 **인간 중심의 순차적 변환**을 전제로 하지만, 대규모 언어 모델(LLM)과 멀티모달 AI는 근본적으로 다른 방식으로 작동한다.

![AI 지능 프레임워크: DIKW를 넘어선 새로운 패러다임](images/ds-continuum-beyond-dikw.svg){#fig-beyond-dikw fig-align="center"}

@fig-beyond-dikw 는 2025년 AI 시대에 맞춰 재정의된 지능 생성 프레임워크를 시각화한다. 전통적 DIKW의 4단계를 AI 중심으로 재해석한 새로운 계층 구조다.

### Raw Signals: 비구조화 데이터의 직접 처리 {.unnumbered}

전통적 데이터 개념은 구조화된 숫자와 기호를 전제했지만, 현대 AI는 **비구조화 원시 신호(Raw Signals)**를 직접 처리한다. 텍스트, 이미지, 오디오, 비디오 등 다양한 모달리티의 데이터를 사전 정제 없이 입력받아 처리할 수 있다. 이는 전통적 데이터 파이프라인에서 필수였던 ETL(Extract, Transform, Load) 단계를 대폭 간소화한다.

### Embeddings: 의미의 벡터 표현 {.unnumbered}

전통적 정보 개념은 인간이 읽을 수 있는 텍스트나 차트를 의미했지만, AI 시대에는 **임베딩(Embeddings)**이 핵심이다. 원시 신호를 고차원 벡터 공간의 점으로 변환하여, 의미적 유사성을 수학적으로 계산 가능하게 만든다. 이를 통해 검색, 분류, 추천 등 다양한 작업을 벡터 연산으로 처리할 수 있다. Pinecone, Weaviate 같은 벡터 데이터베이스가 이 계층의 핵심 기술이다.

### Context: 프롬프트 엔지니어링의 시대 {.unnumbered}

DIKW에는 없었던 완전히 새로운 계층이 **맥락(Context)**이다. 대규모 언어 모델의 출력은 입력 프롬프트의 구성 방식에 극도로 민감하다. Few-shot learning, Chain-of-Thought prompting, Retrieval-Augmented Generation(RAG) 등의 기법을 통해 모델에게 적절한 맥락을 제공하는 것이 성능을 결정한다. 이는 전통적 데이터 과학에서는 존재하지 않았던 새로운 전문 영역이다.

### Intelligence: 창발적 추론 능력 {.unnumbered}

전통적 지식 개념은 명시적으로 학습되고 저장된 정보를 의미했지만, AI의 **지능(Intelligence)**은 창발적(Emergent)이다. 모델은 명시적으로 프로그래밍되지 않은 추론 체인(Reasoning Chains)을 생성하고, 학습 데이터에 없던 패턴을 합성(Pattern Synthesis)하며, 완전히 새로운 콘텐츠를 창의적으로 생성(Creative Generation)한다. 이는 LangChain, LlamaIndex 같은 LLM 프레임워크를 통해 구현된다.

### Emergent Capabilities: 동적 적응과 에이전트 행동 {.unnumbered}

전통적 지혜 개념은 인간의 가치 판단을 전제했지만, AI 시대에는 **창발적 역량(Emergent Capabilities)**이 등장한다. 모델은 동적으로 적응(Dynamic Adaptation)하고, 지속적으로 학습(Continuous Learning)하며, 자율적인 에이전트 행동(Agentic Behavior)을 보인다. AutoGen, CrewAI 같은 멀티 에이전트 오케스트레이션 도구가 이를 가능하게 한다.

### 패러다임 전환의 본질 {.unnumbered}

@fig-beyond-dikw 의 우측 패널은 4가지 근본적인 패러다임 전환을 보여준다:

1. **저장에서 생성으로(From Storage to Generation)**: 데이터를 저장하고 조회하는 것이 아니라, 필요한 시점에 생성한다.
2. **쿼리에서 대화로(From Queries to Conversations)**: 구조화된 쿼리 언어가 아니라 자연어 대화로 상호작용한다.
3. **규칙에서 패턴으로(From Rules to Patterns)**: 명시적 규칙 기반이 아니라 데이터에서 학습한 패턴 기반으로 작동한다.
4. **정적에서 적응적으로(From Static to Adaptive)**: 고정된 시스템이 아니라 지속적으로 학습하고 진화하는 시스템이다.

중앙의 **Foundation Models**(기초 모델)는 이 모든 계층을 연결하는 핵심이다. GPT-4, Claude, Gemini 같은 대규모 언어 모델과 멀티모달 AI가 전체 프레임워크의 중심 엔진 역할을 하며, 인간과 AI의 협업(Human-AI Collaboration)을 통해 완성된다.

DIKW는 여전히 유효한 개념적 프레임워크지만, AI 시대 데이터 과학자는 **두 프레임워크 모두를 이해**해야 한다. 전통적 DIKW는 "왜 데이터가 중요한가?"를 설명하고, 새로운 AI 프레임워크는 "어떻게 AI가 데이터를 처리하는가?"를 설명한다. 둘의 통합적 이해가 현대 데이터 과학의 핵심 역량이다.

다음 장에서는 AI 시대 데이터 과학자의 역할과 필요한 역량을 구체적으로 살펴본다.

## 데이터 과학 워크플로우 {#sec-data-science-workflow}

이론적 프레임워크를 이해했다면, 이제 실제 데이터 과학 프로젝트가 어떻게 진행되는지 살펴볼 차례다. 데이터 과학 워크플로우는 **현실 세계(Real World)**에서 시작하여 다시 현실 세계로 돌아가는 순환적 프로세스다.

![데이터 과학 워크플로우: 순환적 프로세스](images/ds-continuum-data-science-workflow.svg){#fig-data-science-workflow fig-align="center"}

@fig-data-science-workflow 는 데이터 과학의 전형적인 워크플로우를 시각화한다. 이는 앞서 살펴본 7단계 연속체와 DIKW 계층 구조를 실무적 관점에서 재구성한 것이다.

### 1단계: 원시 데이터 수집(Raw Data is Collected) {.unnumbered}

현실 세계에서 발생하는 현상을 데이터로 포착하는 단계다. 센서 데이터, 트랜잭션 로그, 사용자 행동, 설문 응답 등 다양한 형태의 원시 데이터를 수집한다. 이 단계에서는 데이터의 완전성(Completeness)과 정확성(Accuracy)보다는 **포괄성(Comprehensiveness)**이 중요하다. 나중에 필요할 수 있는 데이터를 미리 수집하는 것이 원칙이다.

### 2단계: 데이터 처리(Data is Processed) {.unnumbered}

수집된 원시 데이터를 분석 가능한 형태로 변환하는 단계다. 데이터 클리닝(결측치 처리, 이상치 제거), 데이터 변환(정규화, 표준화, 인코딩), 데이터 통합(여러 소스 결합), 피처 엔지니어링(새로운 변수 생성) 작업을 수행한다. 실무에서는 이 단계가 전체 프로젝트 시간의 **60-80%**를 차지한다.

### 3단계: 정제된 데이터(Clean Data) {.unnumbered}

처리 과정을 거쳐 분석 준비가 완료된 데이터다. 일관된 형식, 완전한 정보, 검증된 품질을 갖춘 상태로, 이후 모든 분석 작업의 기반이 된다. 정제된 데이터는 여러 분석 경로로 분기된다.

### 4단계: 탐색적 데이터 분석(Exploratory Data Analysis, EDA) {.unnumbered}

데이터의 특성과 패턴을 파악하기 위한 초기 탐색 단계다. 기술 통계(평균, 분산, 분포), 시각화(히스토그램, 산점도, 박스플롯), 상관관계 분석, 이상치 탐지를 통해 데이터에 대한 직관을 형성한다. Python의 pandas, matplotlib, seaborn이나 R의 ggplot2, dplyr 같은 도구가 핵심이다.

### 5단계: 머신러닝 알고리즘/통계 모델(Machine Learning Algorithms/Statistical Models) {.unnumbered}

EDA를 통해 얻은 인사이트를 바탕으로 예측 모델이나 통계 모델을 구축하는 단계다. 회귀 분석, 분류, 군집화, 차원 축소 등 다양한 알고리즘을 실험하고, 교차 검증(Cross-validation)을 통해 모델 성능을 평가한다. 이 단계의 결과는 커뮤니케이션 단계로 전달된다.

### 6단계: 커뮤니케이션 - 시각화/결과 보고(Communicate Visualizations/Report Findings) {.unnumbered}

분석 결과를 이해관계자에게 전달하는 단계다. 대시보드, 보고서, 프레젠테이션을 통해 데이터 인사이트를 효과적으로 전달한다. 기술적 상세보다는 **비즈니스 임팩트**에 초점을 맞추며, 스토리텔링 기법을 활용하여 설득력을 높인다. 이 단계에서는 EDA 결과와 모델링 결과가 모두 통합된다.

### 7단계: 의사결정(Make Decisions) {.unnumbered}

분석 결과를 바탕으로 실제 비즈니스 결정을 내리는 단계다. 마케팅 전략 수정, 제품 개선, 리스크 관리, 자원 배분 등 구체적인 액션 아이템이 도출된다. 중요한 것은, 이 단계의 결정이 다시 현실 세계에 영향을 미치고(점선 화살표), 그 결과가 새로운 데이터로 수집되어 **순환적 피드백 루프**를 형성한다는 점이다.

### 8단계: 데이터 제품 구축(Build Data Product) {.unnumbered}

분석을 일회성 프로젝트로 끝내지 않고, **지속적으로 가치를 창출하는 제품**으로 발전시키는 단계다. 추천 시스템, 예측 API, 자동화 대시보드, 알고리즘 트레이딩 봇 등 데이터 기반 서비스를 구축한다. 이 제품 역시 현실 세계와 상호작용하며(점선 화살표), 사용자 피드백과 새로운 데이터를 통해 지속적으로 개선된다.

### 피드백 루프의 중요성 {.unnumbered}

@fig-data-science-workflow 의 핵심은 두 개의 점선 화살표로 표현된 **피드백 루프**다. 의사결정과 데이터 제품이 현실 세계에 미치는 영향이 다시 원시 데이터로 수집되어, 워크플로우가 반복적으로 개선된다. 이는 데이터 과학이 단순한 분석 작업이 아니라 **지속적 학습과 개선의 프로세스**임을 보여준다.

전통적 워크플로우에서는 각 단계가 수동으로 진행되었지만, AI 시대에는 이 프로세스의 많은 부분이 자동화되고 있다. 특히 데이터 처리, EDA, 모델링 단계에서 LLM 기반 도구들이 코드 생성, 자동 시각화, 하이퍼파라미터 튜닝을 지원하여 워크플로우를 가속화한다.

다음 장에서는 AI 시대 데이터 과학자의 역할과 필요한 역량을 구체적으로 살펴본다.
